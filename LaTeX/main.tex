\documentclass[12pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{natbib}
\usepackage{hyperref} % garder en dernier
\usepackage{svg}
\usepackage{float}

\svgsetup{
  inkscapelatex=true,
  inkscapeexe=inkscape,
  clean=true
}
% Set a default search path for svg figures (so \includesvg{plot_silence_ca_pousse} works)
\svgpath{{photo/}}
% Autoref names (français) — évite les warnings de hyperref lorsque
% on utilise \autoref sur des environnements non-standards
\providecommand{\theoremautorefname}{Théorème}
\providecommand{\lemmaautorefname}{Lemme}
\providecommand{\propositionautorefname}{Proposition}
\providecommand{\definitionautorefname}{Définition}
\providecommand{\sectionautorefname}{Section}
\providecommand{\subsectionautorefname}{Sous-section}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{bbm}

\geometry{
    left=1.cm,
    right=1.cm,
    top=1.5cm,
    bottom=1.5cm
}

\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}

\newcommand{\Exp}{\mathrm{Exp}}

\title{Analyse du $\Lambda$-coalescent : renouer avec ses racines. }
\author{SALA Raphaël, MUGISHA Axcel, GARCIA Hugo, COLIN Thibault}
\makeatletter
\newcommand{\monthandyear}{%
    \ifcase\month\or janvier\or février\or mars\or avril\or mai\or juin\or juillet\or août\or septembre\or octobre\or Novembre\or Décembre\fi\ \number\year}
\makeatother
\date{\monthandyear}




\begin{document}

\maketitle

    \section{Introduction}
    \subsection{Fondements du $\Lambda$-coalescent}

    La théorie de la coalescence modélise le phénomène par lequel des
    individus d'une population partagent un ancêtre commun. Nous souhaitons 
    étudier rétrospectivement leur évolution.

    Historiquement,
    le modèle de Wright-Fisher étudie une population de taille finie $N$
    où les individus d'une génération coalescent de manière uniforme entre 
    eux dans la génération précédente \cite{Fisher1930}.
    Ensuite, le modèle de Kingman \cite{Kingman1982} est le modèle
    limite de Wright-Fisher où l'on s'intéresse à $n<N$ lignées
    et en considérant $N\to +\infty$.
    Ce cadre asymptotique permet de
    simplifier grandement l'étude du phénomène de coalescence. 
    Le modèle peut à présent être décrit comme un processus de Markov.

    En 1999, Pitman et Sagitov
    généralisent le modèle de Kingman en autorisant la coalescence
    simultanée de plusieurs lignées. Des individus peuvent engendrer 
    une proportion non négligeable de la population. 
    Afin de définir un modèle, nous supposons raisonnablement
    que les lignées coalescent aléatoirement et indépendamment 
    de leur histoire passée,
    c'est-à-dire en supposant l'absence de mémoire (propriété de Markov),
    que toutes les lignées ont les mêmes chances de coalescer entre elles
    que l'on appelle l'échangeabilité et enfin que nous ayons 
    l'absence de collisions multiples signifiant qu'à tout instant donné,
    il ne peut y avoir qu'un seul événement de fusion en un même ancêtre.


    \begin{theorem}[Pitman-Sagitov {\cite{Pitman1999,Sagitov1999}}]\label{thm:pitman-sagitov}
    Il existe un processus de Markov, $(N_t)_{t\geq 0}$, appelé $\Lambda$-coalescent,
    échangeable à collisions multiples simples si et seulement s'il
    existe une mesure finie $\Lambda$ sur $[0,1]$ telle que, lorsqu'on a $b$ lignées,
    pour tout $2\le k\le b$ le taux auquel chaque $k$-uplet fixé de lignées
    fusionne vaut, 
    \[
    \lambda_{b,k}=\int_0^1 x^{k-2}(1-x)^{b-k}\,\Lambda(dx)
    \]
    \end{theorem}

    Nous ne définissons pas formellement les conditions ici et donnons
    encore moins une preuve car cela est au-delà du cadre de ce rapport.
    Ce résultat montre que la dynamique de $(N_t)_{t\geq 0}$,
    indiquant le nombre de lignées à l'instant $t$, est entièrement caractérisée
    par une mesure finie. Sans perte de généralité nous considérons pour la suite
    une mesure de probabilité, $\Lambda$ sur $[0,1]$. 
    Partant de $b$ lignées,
    le taux d'une $k$-coalescence ($2\leq k \leq b$) est 
    $r_{b,k} \coloneq \binom{b}{k} \lambda_{b,k}$. 
    Le taux de sortie de l'état $b$ est la somme des taux donc
    
    \begin{equation} \label{eq:lambda_b}
        \lambda_b = \sum_{k=2}^{b} r_{b,k} = \int_0^1 S_b(x) \,\Lambda(dx),
        \quad
        S_b(x) \coloneq \sum_{k=2}^{b} \binom{b}{k} x^{k-2}(1-x)^{b-k}
        = \frac{1-(1-x)^b - b x (1-x)^{b-1}}{x^2}
    \end{equation}
    
    D'après le lemme des réveils, à chaque événement de coalescence on passe de $b$ à $b-k+1$ lignées
    avec probabilité, 
    \[
    \forall b\geq k \geq 2,
    \quad p_{b,k} \coloneq \frac{r_{b,k}}{\sum_{k=2}^b r_{b,k}} = \frac{\binom{b}{k} \lambda_{b,k}}{\lambda_b}
    \]
    Ainsi, le squelette du processus est une chaîne de Markov
    décroissante sur $\llbracket 1, n \rrbracket$, commençant en $n$ et
    absorbée presque sûrement en $1$.

    \subsection{Exemple (Kingman)} \label{sec:kingman-example}
        Intéressons-nous au modèle de Kingman en guise d'introduction. 
    On pose $\Lambda = \delta_0$.
    Pour $2\leq k\leq b $,
    \[
    \lambda_{b,k}
        = \int_0^1 x^{k-2}(1-x)^{b-k} \delta_0(x) dx
        = [x^{k-2}(1-x)^{b-k}]_{x=0} 
        =
    \begin{cases}
    (1-0)^{b-2} = 1 & \text{si } k = 2 \\
    0^{k-2}(1-0)^{b-k} = 0 & \text{si } k > 2
    \end{cases}
    \]

    Les coalescences ne se font que par paires. Une caractéristique intéressante
    du $\Lambda$-coalescent
    est le TMRCA (Time to the Most Recent Common Ancestor),
    c'est-à-dire le plus petit temps tel que toutes les lignées
    ont fusionné en un ancêtre commun. Dans la suite du rapport, nous le notons
    \[
    \tau_{\Lambda,n} \coloneq \inf \{ t\geq 0, N_t=1 \} \mid \{N_0 = n\}
    \]
    Lorsque le contexte est clair sur $\Lambda$ ou $n$, ceux-ci seront omis 
    afin de rendre la lecture plus agréable.
    
    \begin{lemma}\label{lem:recurrence-TMRCA}
        % Notons le TMRCA d'un $\Lambda$-coalescent $\tau\coloneq \inf \{ t\geq 0, N_t=1 \}$.
        Soit $\Lambda$ une mesure de probabilité sur $[0,1]$. Notons
        $H: b \in \mathbb N^* \longmapsto \mathbb{E}(\tau_{\Lambda, b})$.
        Alors $H(1)=0$, $H(2)=1$ et pour $b\geq 3$,
        \[
        H(b) = \frac{1}{\lambda_b} \;+\; \sum_{k=2}^{b-1} p_{b,k}\, H(b-k+1)
        \]
    \end{lemma}

    \begin{proof}
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$
    dont nous omettons sa présence dans les notations.
    Pour $b=1$, $N_t=1$  donc $H(1)=0$.
    Pour $b=2$, le seul saut possible est de $2$ vers $1$ lignée
    avec taux $\lambda_2=\binom{2}{2}\lambda_{2,2}=1$, d'où $\tau_{2} \sim \Exp(1)$ et $H(2)=1/1=1$.

    Fixons $b\ge 3$. Définissons le temps de la première coalescence,
    \[
    T_b^1:=\inf\{t\geq0, N_t\neq b\} \mid \{N_0 = b\}
    \]

    $(N_t)_{t\geq 0}$ est un processus de Markov avec un taux de saut $\lambda_b$,
    donc $T_b^1\sim \Exp(\lambda_b)$
    et donc $\mathbb{E}(T_b^1)=\frac{1}{\lambda_b}$.
    De plus, si $K$ est la taille de la fusion au temps $T_b^1$, alors 
    $
    K \sim \sum_{k=2}^{b} p_{b,k} \delta_k
    $
    et $N_{T_b^1}=b-K+1$.

    Considérons la filtration naturelle $(\mathcal{F}_t)_{t\geq 0}$
    de $(N_t)_{t\geq 0}$. Par la propriété de Markov forte au temps $T_b^1$ et
    l'absence de mémoire,
    \[
        \mathbb E(\tau_b-T_b^1\mid \mathcal F_{T_b^1})
        =\mathbb E(\tau_{N_{T_b^1}})
        =H(N_{T_b^1})
    \]
    
    Ainsi en conditionnant par $\mathcal{F}_{T_1}$,
    \begin{align*}
        H(b)
        &=\mathbb E(\tau_b)
        =\mathbb E(T_b^1)+\mathbb E(\tau_b-T_b^1)
        % &=\mathbb E_b[T_1]+\mathbb E_b\!\big[\mathbb E_b[\tau-T_1\mid \mathcal F_{T_1}]\big]\\
        =\frac{1}{\lambda_b}+\mathbb E \bigl( \mathbb E(\tau_b-T_b^1\mid \mathcal F_{T_b^1})  \bigr)
        =\frac{1}{\lambda_b}+\mathbb E(H(N_{T_b^1})) \\
        &=\frac{1}{\lambda_b}+\sum_{k=2}^{b}\mathbb P\big(N_{T_b^1}=b-k+1\big) H(b-k+1)
        =\frac{1}{\lambda_b}+\sum_{k=2}^{b} p_{b,k} H(b-k+1)
    \end{align*}

    Or $H(1)=0$, donc le terme $k=b$ s'annule. D'où le résultat.
    \end{proof}

    
    
    
    Pour $b$ lignées observées, on a
    $ \lambda_b = \sum_{k=2}^{b} \binom{b}{k} \lambda_{b,k}
               = \binom{b}{2} \lambda_{b,2}
               = \binom{b}{2}
        $
    donc, d'après le \autoref{lem:recurrence-TMRCA}, la taille moyenne d'un arbre pour
    le modèle de Kingman est donnée par,
    \[
    H(b) = \frac{1}{\lambda_b} + p_{b,2}\,H(b-1)
    = \frac{1}{\binom{b}{2}} + H(b-1)
    \]

    Ainsi par récurrence,
    \begin{equation}\label{eq:tmrca-kingman}
        H(b)=\sum_{k=2}^{b}\frac{1}{\binom{k}{2}}
        = \sum_{k=2}^{b}\frac{2}{k(k-1)}
        = \sum_{k=2}^{b}2\Big(\frac{1}{k-1}-\frac{1}{k}\Big)
        =2\Big(1-\frac{1}{b}\Big)
    \end{equation}
    
    
    Nous illustrons ce résultat par une simulation pour $n=20$ lignées (\autoref{fig:kingman_simu}). 
    On observe bien que les fusions sont binaires. L'état $1$ est absorbant
    et dans ce cas ci le TMRCA est supérieur à $\tau_{\delta_0,n}$.
    La grande partie des coalescences se fait dans les premiers instants.
    En référence au lemme des réveils, le nombre de paires possibles, et donc 
    de réveils prêts à sonner et paramétrés de la même manière, est plus grand 
    au début qu'à la fin.
    L'arbre a donc l'aspect d'un acacia de la savane \footnote{Référence au Vachellia tortilis, qui 
    n'est plus considéré comme un acacia.}, dense et large en haut, fin en bas.
    % Enfin, chaque paire de lignées a son propre réveil et celui 
    % qui sonne en premier créé une coalescence, ainsi de par 
    % le grand nombre de réveils au début le nombre de lignées 
    % dimininue conséquemment. Cette décroissance est exponentielle
    % dans ce le modèle de Kingman et cet aspect ne sera pas plus développé 
    % dans la suite du rapport pour le cas des $\Lambda$-coalescent.
    % et que le temps total de coalescence oscille autour de la valeur théorique $2(1 - 1/20) = 1.9$.

\begin{figure}[H]
    \centering
    \includesvg[width=\textwidth]{photo/plot_arbre_kingman}
\caption{Réalisation du coalescent de Kingman issu de $n=20$ lignées.
À gauche, représentation de l'arbre généalogique et mesure du TMRCA comparé à la
valeur théorique $\tau_{\delta_0,n}= 2(1-1/20)=1.9$.
À droite, évolution du nombre de lignées $N_t$ jusqu'à l'état absorbant $N_t=1$ où 
nous atteignons un plateau constant pour $t > TMRCA$.}
    \label{fig:kingman_simu}
\end{figure}
    
%\textcolor{red}{
%    Subplots (kingman)
%    \begin{itemize}
%        \item (Une réalisation) Arbre + TMRCA 
%        \item (Sur plusieurs réalisaiton) Distribution des fusions ((C\_t\^{}i))\_\{0<i<n \} (donc uniquement en 2 normalement)
%        \item  (Sur plusieurs réalisation) distribution TMRCA + densité avec en légende "densité théorique"
%        dans le caption mettre, ou en footnote, que la densité est explcite dans ce cas dont la formule 
%        n'est pas prouvée, ele peut être écritre matriciellement d'après (citer "une foret pas si grande")
%    \end{itemize}
%}

    \section{Analyse du TMRCA}
    \subsection{Aux extrêmes de l'arbre. }

    Au vu du précédent exemple, on peut se demander l'influence de la mesure
    $\Lambda$ sur le TMRCA. Intuitivement, ce temps moyen devrait diminuer lorsque
    la masse de $\Lambda$ se rapproche de 1 puisqu'on autorise des coalescences
    multiples plus importantes. En première analyse on va étudier
    les deux cas extrêmes.

    % \begin{lemma}\label{lem:bounds-Sb}
    %     Pour tout $b\geq 2$ et $x\in[0,1]$,
    %     \[
    %     1 \leq S_b(x) \leq \binom{b}{2}
    %     \]
    % \end{lemma}
    % \begin{proof}
    %     Soit $(X_i)_{1\geq i \geq k}$
    %     de loi de Bernouilli de paramètre $x \in [0,1]$ indépendantes.

    %     Posons $X\coloneq\sum_{i=1}^b X_i \sim \mathrm{Bin}(b,x)$. On a,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \mathbb{E}(\sum_{i\neq j} X_i X_j)
    %     \overset{\perp\!\!\!\perp}{=}\sum_{i\neq j} \mathbb{E}(X_i)\mathbb{E}(X_j)  
    %     = b(b-1)x^2
    %     \]

    %     En utilisant que $k(k-1) \geq 2 \mathbbm{1}_{k\geq 2}$,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \sum_{k=0}^b k(k-1) \binom{b}{k} x^k (1-x)^{b-k}
    %     \geq \sum_{k=2}^b 2 \binom{b}{k} x^k (1-x)^{b-k}
    %     = 2 S_b(x)x^2
    %     \]

    %     D'où,
    %     \[
    %     2S_b(x)x^2 \leq b(b-1)x^2 
    %     \Longleftrightarrow
    %     S_b(x) \leq \frac{b(b-1)}{2} = \binom{b}{2}
    %     \]

    %     Pour l'autre inégalité, on remarque que pour tout $x\in]0,1]$,
    %     \[
    %     S_b(x) = \frac{1}{x^2}\mathbb P(X\geq 2)
    %     \geq \frac{1}{x^2} \mathbb{P}(X_1=X_2=1) = \frac{x^2}{x^2} = 1
    %     \]

    %     Puis $S_b(0) = \binom{b}{2} \geq 1$, d'où le résultat.
    % \end{proof}

    \begin{proposition}
        Soit $n$ le nombre de lignées. 
        % Notons le TMRCA d'un $\Lambda$-coalescent,
        % \[
        % \tau\coloneq \inf \{ t\geq 0, N_t=1 \}
        % \]
        Alors, pour toute mesure de probabilité $\Lambda$ sur $[0,1]$, on a,
        \[
        1 = \mathbb{E}(\tau_{\delta_1,n}) 
        \leq \mathbb{E}(\tau_{\Lambda,n})
        % \leq\mathbb{E}_{\delta_0}(\tau) = 2\left(1-\frac{1}{n}\right)
        \]
    \end{proposition}
    \begin{proof}
    % L'égalité de droite a été montrée dans l'exemple de Kingman (voir \autoref{sec:kingman-example}).
    Prouvons l'égalité. Prenons $\Lambda = \delta_1$, nous avons $\lambda_{n,k} = \delta_{n,k}$ (symbole de Kronecker),
    donc $\lambda_n = \binom{n}{n} \lambda_{n,n} = 1$ donc
    $\tau_{\delta_1} \sim \Exp(1)$ et donc $\mathbb{E}(\tau_{\delta_1}) = 1/1 = 1$.
    
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$.
    Notons $H(b) \coloneq \mathbb{E}(\tau_{\Lambda, b})$.
    % D'après \eqref{eq:lambda_b}, 
    % D'après la propriété de Markov et l'absence de mémoire de l'exponentielle,
    % \[
    % H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    % \]
    
    Montrons par récurrence forte l'inégalité, c'est-à-dire $H(b)\geq 1$
    pour $b\geq 2$.
    % Pour $b=2$, $\lambda_{2,2}=1$ donc $\tau \sim \Exp(1)$
    % donc $H(2) = 1 \geq 1$. 
    L'initialisation a été prouvée dans le lemme \ref{lem:recurrence-TMRCA}.
    Supposons l'inégalité vraie jusqu'à $b-1$. 
    Remarquons que $\lambda_{b,b} = \int_0^1 x^{b-2}\,\Lambda(dx) \leq \int_0^1 \Lambda(dx) = 1$,
    \[
    H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    \geq \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} 
    = \frac{1}{\lambda_b} + 1 - p_{b,b}
    = 1 + \frac{1 - \lambda_{b,b}}{\lambda_b} \geq 1
    \]

    D'où le résultat.
    \end{proof}

    Cette idée de déplacer la masse de $\Lambda$ vers 1 pour diminuer la moyenne 
    du TMRCA
    est intuitive. Pour le problème inverse de maximisation du TMRCA nous
    souhaiterions déplacer la masse de $\Lambda$ vers 0. C'est-à-dire prouver que 
    le modèle de Kingman soit celui maximisant le temps moyen du TMRCA. 
    Toutefois, voilà une grande surprise : ce n'est pas le cas !

    \begin{proposition}\label{prop:surprise-TMRCA}
        Il existe $n>1$ et une mesure de probabilité $\Lambda$ sur $[0,1]$
        telle que, 
        \[
        \mathbb{E}(\tau_{\Lambda,n})> \mathbb{E}(\tau_{\delta_0,n})
        \]
    \end{proposition}
    \begin{proof}
        Soit $n=8$, dans l'exemple de Kingman (voir \autoref{sec:kingman-example}),
        nous avons une formule explicite.
        \[\
        \mathbb E(\tau_{\delta_0}) = 2\left(1-\frac{1}{8}\right) = \frac{14}{8} = 1.75
        \]

        Soit $\Lambda = \delta_{1/4}$, alors d'après \eqref{eq:lambda_b},
        \[ 
        \lambda_{n,k} = \left(\frac{1}{4}\right)^{k-2} \left(\frac{3}{4}\right)^{n-k}
        \quad 
        \lambda_n = 16\left(1 -\left(\frac34\right)^n - \frac{n}{4}\left(\frac34\right)^{n-1}\right)
        \] 
        
        Ainsi, en calculant nous obtenons, toujours pour $n=8$,
        \[ 
        \mathbb{E}(\tau_{\delta_{1/4},n})
        = \frac{1}{\lambda_n} + \sum_{k=2}^{n-1} \frac{\binom{n}{k} \lambda_{n,k}}{\lambda_n} \mathbb{E}(\tau_{\delta_{1/4},n-k+1} )
        = \frac{19954284839411683}{11337879079537330} \approx 1.7599662 \dotsc > 1.75
        \]
    \end{proof}
    Nous conjecturons que le théorème peut être étendu pour tout $n>6$.
    A notre connaissance l'étude ce phénomène n'est pas documenté pour $n$ fini.
    Seul un article de \cite{kluge2017exchangeable} s'intéresse
    la croissance de $ \sup_\Lambda \mathbb{E}(\tau_\Lambda)$ lorsque $n\to\infty$.


      \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_tmrca_extrema_histograms}
        \caption{Distribution empirique du TMRCA pour $n=8$
        de $\Lambda=\delta_0$ (Kingman, histogramme vers le haut en bleu)
        et $\Lambda=\delta_{1/4}$ (histogramme vers le bas en orange).
        Les bandes verticales pointillées délimitent les intervalles
        de confiance (IC) à $95\%$ pour la moyenne de
        $\tau$ basés sur $M=1e5$ simulations
        indépendantes. Les lignes pleines indiquent les moyennes
        empiriques et les lignes pointillées ("-.")
        les moyennes théoriques.}
        \label{fig:tmrca-extrema}
    \end{figure}

    Dans la \autoref{fig:tmrca-extrema} nous utilisons des intervalles
    de confiances.
    Soit $(T_i)_{1\leq i \leq M}$ une collection de variables 
    aléatoires i.i.d. de loi $\tau_\Lambda$. Posons la moyenne et
    l'estimateur de la variance
    \[
        \overline T_M \coloneq \frac{1}{M}\sum_{i=1}^M T_i\qquad
        s_M^2 \coloneq \frac{1}{M-1}\sum_{i=1}^M (T_i - \overline T_M)^2
    \]
    
    Les intervalles de confiance sont construits génériquement.
    D'après le théorème central limite et le lemme de Slutsky,
    \[
        \sqrt{M}\frac{\overline T_M -
        \mathbb{E}_\Lambda[\tau]}{\sqrt{s^2_M}}
        \xrightarrow[M\to\infty]{\mathcal{L}} \mathcal N(0,1)
    \]

    Ainsi, un IC asymptotique de niveau $1-\alpha$ est
    \[
        [ \overline T_M \pm q_{1-\alpha/2}
        \frac{\sqrt{s^2_M}}{\sqrt{M}}]
    \]

    Dans la \autoref{fig:tmrca-extrema} 
    le zoom à droite montre les intervalles de confiance à $95\%$ disjoints
    renforçant l'observation $\mathbb E(\tau_{\delta_0, 8}) <
    \mathbb E(\tau_{\delta_{1/4}, 8})  $.


%    \textcolor{blue}{L'échelle de temps ici est en unités de $N$ générations, avec $N\gg n$ puisque
%     nous considérons un modèle asymptotique.}

    % TODO : Reformuler !
    % Une suprise ... À notre connaissance,
    % la question de déterminer, pour un (n) fixé,
    % la mesure (\Lambda) qui maximise l’espérance du temps
    % d’absorption (\mathbb E_\Lambda[\tau]) du (\Lambda)-coalescent
    % reste ouverte. Les travaux existants se concentrent sur les lois
    % limites et les constantes asymptotiques de (\tau_n) lorsque
    % (n\to\infty) (voir p.ex. Kersting–Wakolbinger), sans adresser
    % l’extrémalité en (\Lambda) pour (n) fini.


    \subsection{Une forêt pas si grande } \label{sec:une-foret-pas-si-grande}

    Un processus de Markov est entièrement déterminé
    par son générateur infinitésimal.
    Pour $n$ lignées observées,
    celui d'un $\Lambda$-coalescent
    est la matrice triangulaire inférieure
    $Q \in \mathcal M_n(\mathbb{R})$ définie 
    pour tout $1\leq b,i\leq n$, par
    \[
        Q_{b,i} =
        \begin{cases}
        r_{b,k} & \text{si } b\ge 2 \text{ et } i = b-k+1 \text{ pour } 2\le k\le b\\
        -\lambda_b & \text{si } b\ge 2 \text{ et } i = b\\
        0 & \text{sinon}
        \end{cases}
    \]

    Le premier élément de sa diagonale, $Q_{1,1}$, est nul
    car l'état $1$ est absorbant donc $Q$ n'est pas inversible.
    En se restreignant à la sous-matrice $R = (Q_{i,j})_{2\leq i,j \leq n}$
    la matrice devient inversible et nous pouvons exprimer la densité de $\tau$.
    Posons $p_R(t) = (p_k(t))_{2\leq k \leq n}$
    où $p_k : t \geq 0 \mapsto \mathbb{P}(N_t = k \mid N_0=n)$. D'après
    la relation de Chapman-Kolmogorov, $p_R$ vérifie pour tout $t\geq 0$
    \[
    \begin{cases}
        p_R'(t) = p_R(t) R \\
        p_R(0) = (0,\dots,0,1)
    \end{cases}
    \iff p_R(t) = (0,\dots,0,1) e^{tR}
    \]
    Définissons la fonction de survie,
    $S : t \mapsto \mathbb{P}(\tau_n > t) = \mathbb{P}(N_t\neq 1 \mid N_0=n)
    = \sum_{k=2}^n p_k(t) = p_R(t) \cdot \textbf{1}$.
    Donc la densité de $\tau_n$ est donnée par, 
    \begin{equation}\label{eq:density-TMRCA}
        f_\tau : t\mapsto d_t (1-S(t))
        = -S'(t) = -p_R'(t) \cdot \textbf{1}
        = -p_R(t) R \cdot \textbf{1}
        = - (0,\dots,0,1) e^{tR} R \cdot \textbf{1}
    \end{equation}
   


    On remarque également que ce processus est défini 
    par $(\lambda_{b,k})_{I_n}$ avec
    $I_n \coloneq \{(b,k), 2\leq k \leq b \leq n \}$.
    Définissons pour $r \in \llbracket 0, n-2 \rrbracket$
    \[ 
    m_r : \Lambda \mapsto \int_0^1 x^r \Lambda(dx)
    \]

    En développant l'intégrande des taux de fusions, 
    pour tout $(b,k) \in I_n$,
    il existe $A_n \in \mathcal M_{I_n, n-1}(\mathbb{R})$ tel que, 
    \[
    \lambda_{b,k} = \sum_{r=0}^{n-2} A_{(b,k),r} m_r(\Lambda)
    \]
        
    Pour $n$ fixé, $Q$ est entièrement déterminée
    par $(m_r(\Lambda))_{0\leq r \leq n-2}$,
    l'espace
    des mesures de probabilité sur $[0,1]$ se réduit
    à une projection
    de dimension finie, $\mathbb{R}^{n-1}$, donc un espace bien plus petit.
    Autrement dit, 
    une infinité de mesures différentes
    deviennent indiscernables pour un processus considéré.
 
\begin{proposition}\label{prop:beta_indiscernable}
    Soit $n>1$ et $\Lambda_0^\alpha \coloneq \mathrm{Beta}(2-\alpha,\alpha)$ avec $\alpha \in ]0,2[$,
    de densité 
    \[w_\alpha : x\in [0,1] \longmapsto
    \frac{1}{B(2-\alpha,\alpha)} x^{1-\alpha}(1-x)^{\alpha-1}
    \]

    Soit $(J_{n})_{n\geq 0}$  les polynômes de Jacobi. 
    Pour tout $n\geq 0,
    J_{n}=\sum_{k=0}^{n}
        \binom{n+\alpha-1}{n-k+1}
        \binom{n-\alpha+1}{k}
        x^{n-k}(x-1)^k$
    est de degré $n$ et orthogonal à tous les polynômes de degré inférieur à $n-1$
    pour
    le produit scalaire 
    \cite{nevai1994generalized} % j'arrive pas à le mettre quelque part de bien

    \[
    \langle f,g\rangle_\alpha=\int_0^1 f(x)g(x) w_\alpha(x) dx 
    \]

      On pose
  \[
    M \coloneq \sup_{x\in[0,1]}|J_{n-1}(x)| \in ]0,+\infty[
    \qquad\text{et}\qquad
    \varepsilon_n := \frac{1}{M}
  \]

  Et on définit pour $0<\varepsilon< \varepsilon_n$, la mesure de
   probabilité $\Lambda_{\varepsilon}^\alpha \neq 
    \Lambda_0^\alpha$ par sa densité
    
    \[
    f_\varepsilon^\alpha : x \in [0,1] \longmapsto 
    \bigl(1+\varepsilon J_{n-1}(x)\bigr) w_\alpha(x)
    \]

    Alors, pour tout $0<\varepsilon<\varepsilon_n$,
    \[
    \tau_{\Lambda_{\varepsilon}^\alpha,n}
    \overset{\mathcal{L}}{=}
    \tau_{\Lambda_{0}^\alpha,n}
    \]
\end{proposition}
\begin{proof}
    Soit $\varepsilon<1/M$, montrons que $f_\varepsilon^\alpha$ est bien une densité.
    Sur $[0,1]$,
    $1+\varepsilon J_{n-1} \ge 1-\varepsilon M \ge 0$
    donc par produit de termes positifs $f_\varepsilon^\alpha \ge 0$.

    Puis, par orthogonalité de $J_{n-1}$ avec la constante
    $1 \in \mathbb{R}_{n-1}[X]$,
    % (comme $1$ est un polynôme de degré inférieur à $n-1$)

    \[
        \int_0^1 J_{n-1}(x) w_\alpha(x) dx = 0
    \]

    D'où,
    \[
        \int_0^1 f_\varepsilon^\alpha(x)dx
        = \int_0^1 w_\alpha(x)dx
          + \varepsilon \int_0^1 J_{n-1}(x)w_\alpha(x)dx
        = 1 + \varepsilon\cdot 0 = 1
    \]

    Ainsi $f_\varepsilon^\alpha$ est bien une densité sur $[0,1]$.

    Montrons à présent que les générateurs infinitésimaux de $\Lambda_0^\alpha$ et
    $\Lambda_\varepsilon^\alpha$ coïncident.
    Pour tout $r \in \llbracket 0, n-2 \rrbracket$, on a
    $X^r \in \mathbb R_{r}[X] \subset \mathbb{R}_{n-1}[X]$, ainsi par 
    orthogonalité de $J_{n-1}$,
    \[
        \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\,dx = 0
    \]

    Ainsi,
    $$
        m_r(\Lambda_\varepsilon^\alpha)
        = \int_0^1 x^r f_\varepsilon(x)\,dx
        = \int_0^1 x^r w_\alpha(x)\,dx
          + \varepsilon \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\,dx
        = m_r(\Lambda_0^\alpha)
    $$

   Les taux de fusions sont donc égaux entre ces mesures,
    d'où le résultat.
\end{proof}


%\begin{proposition}\label{}
%Soit $n>1$. Notons $P_k$ le polynôme de 
%Legendre de degré $k$. 
%Prenons $\Lambda_0$ la mesure uniforme 
%sur $[0,1]$. Pour tout $0<\varepsilon<1$,
%définissons la mesure de probabilité
%$\Lambda_\varepsilon \neq \Lambda_0$
%de densité sur $[0,1]$
%\[ 
%f_\varepsilon(x) = 1 + \varepsilon P_{n-1}(2x-1)
%\]
%
%Alors, pour tout $0<\varepsilon<1$, 
%\[
%\tau_{\Lambda_0} \overset{\mathcal{L}}{=} \tau_{\Lambda_\varepsilon}
%\]

%\end{proposition}
%\begin{proof}
%Montrons que pour tout $0<\varepsilon<1$,
%$f_\varepsilon$ est bien une densité de probabilité.
%Nous avons, par une analyse élémentaire que
%pour tout $k\geq 0$
%$|P_k|\leq 1$ sur $[-1,1]$
%\cite{WhittakerWatson1920} donc $f_\epsilon \geq 1-1\cdot\varepsilon \geq0 $.
%
%Rappelons que $(P_k)_{0\leq k \leq n}$ est une base orthogonale de 
%$\mathbb{R}_n[X]$ pour le produit scalaire 
%$f,g \mapsto \int_{-1}^1 f(x)g(x) dx$
%donc $\int_0^1 f_\varepsilon(x)dx =
%1+\frac{\varepsilon}{2} \int_{-1}^1 1\cdot P_n(x) dx = 1+0 = 1$. 
%Montrons à présent que le générateur infinitésimal
%de $\Lambda_0$ et $\Lambda_\varepsilon$ sont identiques.
%Soit $r \in \llbracket 0, n-2 \rrbracket$,
%puisque $X^r \in \mathbb{R}_{n-2}[X]
%\subset \mathbb{R}_{n-1}[X]$,
%\[
%m_r(\Lambda_\varepsilon)
%= \int_0^1 x^r f_\varepsilon(x) dx
%= \int_0^1 x^r dx + \varepsilon \int_0^1 x^r P_{n-1}(2x-1) dx
%= \frac{1}{r+1} + \varepsilon \cdot 0
%= m_r(\Lambda_0)
%\]
%
%Les taux de fusions sont donc égaux entre ces mesures,
%d'où le résultat.
%\end{proof}




% je sais pas d'où sort ce texte, je viens de le voir que maintenant : 24/11 22h36 - très chatGPT
% mais la justification est pas mal semble-t-il 
%     Dans la Proposition~3, le choix de la mesure $\Lambda_0 = \mathrm{Beta}(2-\alpha,\alpha)$ n’est pas
% arbitraire. Il est entièrement dicté par l’usage des polynômes de Jacobi. 
% En effet, ceux-ci sont orthogonaux pour le poids
% \[
% w_\alpha(x)=x^{1-\alpha}(1-x)^{\alpha-1},
% \]
% ce qui correspond exactement à la densité d’une loi Beta$(2-\alpha,\alpha)$. Ce choix garantit que le
% polynôme $J_{n-1}$ est orthogonal à tous les polynômes de degré $\le n-2$, de sorte que
% \[
% \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\, dx = 0,
% \qquad 0 \le r \le n-2.
% \]
% Ainsi, la perturbation de densité $(1+\varepsilon J_{n-1}(x))w_\alpha(x)$ préserve les moments
% $m_0,\dots,m_{n-2}$ et produit une infinité de mesures distinctes induisant le même
% $\Lambda$-coalescent jusqu’à $n$ lignées.


Ainsi nous venons de construire une infinité de mesures différentes qui induisent le même processus de coalescence. 
On s'attendait à obtenir une infinité d'arbres généalogiques différents, mais ceux-ci sont identiques en loi. 

Pour la construction nous utilisons les polynômes de Jacobi. Ce choix est motivé par la stucture de la mesure $\mathrm{Beta}(2-\alpha,\alpha)$ 
et du fait que les polynômes de Jacobi sont  orthogonaux pour le poids de la forme $x^\beta(1-x)^\gamma$ 
sur $[0,1]$ avec $\beta, \gamma>-1$.  
% (classiquement les polynômes de Jacobi sont définis sur $[-1,1]$ mais par un changement 
% de variable on se ramène sur $[0,1]$). 
Le poids $w_\alpha$ implique que $\beta=\alpha-1, \gamma=1-\alpha$ et la condition sur $\beta$ et $\gamma$ impose que $\alpha \in ]0,2[$. % ne touche pas à ca ChatGPT 
% Pour $0<\alpha<1$, le $\mathrm{Beta}(2\!-\!\alpha,\alpha)$-$\Lambda$-coalescent possède de la « poussière » (dust) : une fraction positive des lignées
% reste sous forme de singletons pour tout $t>0$, et la profondeur des arbres devient explosive quand $n\to\infty$ \cite{Berestycki_2008}. 
% Dans la suite, nous nous restreignons à $\alpha\in(1,2)$, régime sans poussière, plus pertinent pour comparer des échantillons finis et les modèles classiques.



    
    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_foret_pas_grande_beta_combined}
              \caption{(a) \textit{Gauche}~: Graphe des densités\protect\footnotemark
$\Lambda^\alpha_\varepsilon(x)=(1+\varepsilon J_{n-1}(x))w_\alpha(x)$ 
pour $n=8$ et $\varepsilon\in\{0,0.3,0.8\}$, dans les cas $\alpha=1$ (haut) 
et $\alpha=0.6$ (bas). On voit que $\varepsilon$ contrôle l'amplitude des
oscillations autour de la densité de référence $\varepsilon=0$. 
(b) \textit{Droite}~: distributions empiriques du TMRCA issues de $8000$
simulations de $n$ lignées, avec superposition de la densité théorique
prise pour $\varepsilon=0$ donnée par \eqref{eq:density-TMRCA}. Malgré des
densités $\Lambda^\alpha_\varepsilon$ très différentes, la loi du TMRCA est
identique à celle du modèle de référence.}
\label{fig:tmrca_foret}
    \end{figure}

    \footnotetext{Sauf pour $\alpha=0.6$ et $\varepsilon=0.8$ qui n'est pas une densité,
    comme expliqué dans les paragraphes qui suivent.}

    La construction que nous proposons permet de jouer avec les densités selon deux paramètres.
    L'allure générale est dictée par $\alpha$ tandis que $\varepsilon$ intensifie l'amplitude des oscillations.
    Le choix de $\varepsilon$ est contraint d'être inférieur à $\varepsilon_n$.
    D'après \cite{Szego1939}, cette borne est assez restrictive,
    \[
    \varepsilon_n = \Theta_{n\to \infty}\Bigl(\frac{1}{n^{|\alpha-1|}}\Bigr)
    \]
    Dans la \autoref{fig:tmrca_foret} sur les graphiques du bas avec  $\alpha=0.6$,
    nous avons pour $n=8$ que $\varepsilon_n = 0.392062$. On remarque alors 
    que la distribution empirique vérifie bien la théorie pour $\varepsilon=0.2 < \varepsilon_n$
    mais aussi pour $\varepsilon=0.8>\varepsilon_n$. La contrainte sur $\varepsilon$
    est seulement pour maintenir une densité positive. 

    Dans le cas $\alpha=1$, graphiques du haut, nous obtenons la mesure uniforme
    et $\varepsilon_n$
    ne dépend plus de $n$. Toutefois, le choix de $\varepsilon$ est tout de même borné
    par $1$ afin que notre densité reste positive.


    %\begin{figure}[H]
%    \centering
%    \includesvg[width=\textwidth]{photo/plot_foret_pas_gde_beta}
%    \caption{(a) \textit{Gauche}~: comparaison des densités remarquablement
%    différentes
%    sur $[0,1]$, \(f_\varepsilon(x)=1+\varepsilon P_{n-1}(2x-1)\),
%    pour $n=10$~: cas uniforme
%    $\varepsilon=0$ et perturbation de Legendre $\varepsilon=0.5$
%    (b) \textit{Droite}~: distribution empirique du TMRCA,
%     issues de $n$ lignées, à partir
%    de 8000 simulations, avec superposition de la densité théorique
%    donnée par \eqref{eq:density-TMRCA}. Comme attendu les densités se
%    confondent.}
%\end{figure}

   

    \subsection{Silence, ça pousse}
    Précédemment nous avons brièvement parlé de la mesure uniforme en 
    prenant $\Lambda_0^\alpha$ avec $\alpha=1$.
    Ce modèle est connu sous le nom de Bolthausen-Sznitman
    et décèle un résultat incontournable.
    % \begin{theorem}[C. Goldschmidt \& J. B. Martin \cite{goldschmidt-martin-2005}]
    \begin{theorem}[Goldschmidt \& Martin~\cite{goldschmidt-martin-2005}]
        Soit $(N_t)_{t\geq 0}$ un Bolthausen-Sznitman coalescent.
        % Pour $n>0$,
        % $ \tau_n \coloneq \inf\{t\geq 0, N_0 = n, N_t=1\}$. 
        Alors, 

        \[
            \tau_n - \log(\log(n))  \xrightarrow[n\to\infty]{\mathcal{L}} \mathcal G
        \]
        
    où $\mathcal{G}$ est la loi de Gumbel, de densité $x\mapsto e^{-x - e^{-x}}$.

    \end{theorem}
    La preuve est omise car elle dépasse le cadre de ce rapport. Toutefois, 
    ce théorème renforce l'intuition qu'on a pu commencer à avoir
    à la \autoref{prop:surprise-TMRCA} puisqu'on a que,
    \[
    \lim_{n\to \infty}\mathbb{E}(\tau_n)
    = \lim_{n\to \infty} \log(\log(n)) + \mathbb{E}(\mathcal G)
    = \lim_{n\to \infty} \log(\log(n)) + \gamma
    = +\infty 
    \]
    où $\gamma$ est la constante d'Euler-Mascheroni. En effet,
    comme l'illustre la \autoref{fig:silence-ca-pousse},
    dès $n \approx 50$ on observe $\mathbb{E}(\tau_n) > 2$,
    surpassant la borne du modèle de Kingman $\eqref{eq:tmrca-kingman}$.
    Ainsi, la croissance de la hauteur des arbres généalogiques
    pour le modèle de Bolthausen-Sznitman est extrêmement lente 
    mais permet d'obtenir des arbres aussi grands que l'on souhaite 
    en moyenne.
    % Par exemple en se plaçant de la cadre asymptotique sur 
    % le nombre de lignées initiales $n$, 
    % si on souhaite avoir un arbre
    % de Bolthausen-Sznitman plus grand que celui de Kingman, dont sa hauteur
    % est majorée en moyenne par $2$, il faudrait résoudre 
    % $\log(\log(n))+\gamma \geq 2 \iff n \geq e^{e^{2-\gamma}} \approx 80 $

    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_silence_ca_pousse}
        \caption{(a) \textit{Gauche}~:
        sous le modèle de Bolthausen-Sznitman ($\Lambda$ uniforme)
        nous déterminons les moyennes empiriques de
        $\mathbb{E}(\tau_n)$ (500 répétitions par $n \in 
        \{10, 20, 50, 100, 200, 500, 1e3,2e3,5e3,1e4\}$) comparées à
        l'approximation théorique $\gamma+\log\log n$ et aussi à
        Kingman \eqref{eq:tmrca-kingman}. (b) \textit{Droite}~: histogramme de
        $\tau_n-\log\log n$ pour $n=1000$ (5000 simulations) avec
        superposition de la densité de Gumbel $x\mapsto e^{-x-e^{-x}}$.}
        \label{fig:silence-ca-pousse}
    \end{figure}

























\section{Discordances par paires chez la sardine japonaise}

Afin de justifier le modèle du $\Lambda$-coalescent, on peut le comparer à 
son prédécesseur, le coalescent de Kingman,
nous présentons une application en biologie des populations.
Nous nous appuyons sur \cite{Niwa2016} qui étudie la génétique
des sardines japonaises.
Pour chaque paire de sardines, ils ont analysé leur ADN.
En représentant leur séquence comme un mot
issu de l'alphabet $\{A,T,G,C\}$, nous comptons pour les lignées 
$i\neq j$ leur discordance, notée $K_{ij}\in \mathbb N$, c'est le nombre
de symboles différents lorsque l'on compare leur séquence ADN.
Nous notons $T_{ij} \geq 0$ le plus petit temps tel 
que la lignée $i$ et la lignée $j$ ont un ancêtre commun.
Sous des hypothèses biologiques standard, 
on suppose qu'il y a au plus une mutation par site. Avec un paramètre 
de mutation constant $\mu>0$, pour tout $t>0$ \cite{durrett2008pm4dna},

\[
K_{ij} \mid T_{ij=t} \sim \mathrm{Poisson}(\mu t)
\]

Donc,
\begin{equation}\label{eq:esperance-TK}
    \mathbb{E}(K_{ij}) = \mu \mathbb{E}(T_{ij})
\end{equation}


Ainsi pour modéliser l'arbre généalogique de ces 
sardines nous devons déterminer un $\Lambda$ adéquat.
% Une idée est de la choisir de sorte 
% à ce que la distribution empirique du nombre de discordance
% sur les lignées $i$ et $j$ suive la loi de $K_{ij}$.
Une idée est de comparer 
les lois marginales prédites par chaque $\Lambda$-coalescent pour 
$K_{ij}$ à la distribution empirique observée.

% Nous simulons pour certaines mesure candidates $\Lambda$
% des réalisations de $\Lambda$-coalescent. Sur chacune de 
% ces réalisations, nous appliquons des mutations sur chacune 
% des paires de lignées et nous comptons pour chaque paire le nombre 
% de discordance. La mesure qui correspond au mieux aux données 
% est chosie.

Le choix de mesure couramment utilisé en biologie est le 
modèle de Kingman avec expansion.  % source ? gpt.com
C'est-à-dire que l'intensité des coalescences varie 
au cours du temps.
Ce modèle permet d'expliquer des populations où le nombre
de descendants est modéré par rapport à la taille de la population totale.
Nous considérons dans la suite, par simplicité, le 
cas de deux époques. Soit $c_0,c_1>0$ l'intensité des époques et $\theta>0$
le moment de rupture. On définit
$c : t>0 \mapsto c_0 1_{t\leq \theta} + c_1  1_{t> \theta}$
l'intensité de coalescence au cours du temps. Puisque nous 
avons considéré au début du rapport des mesures de probabilités, afin 
de faire varier l'intensité de coalescence de la mesure $\delta_0$, 
nous renormalisons le temps de notre processus selon $s : t\mapsto \int_{0}^{t}c(u)du$
et donc on s'intéresse au processus $(N_{s(t)})_{t\geq 0}$.
En notant $T_K$ le temps de coalescence entre deux lignées
sous ce modèle, nous avons,
\[
\mathbb{P}(T_K > t) = \exp\Bigl(-\int_0^t c(u) du\Bigr)
= \exp(-c_0 \min(t,\theta) - c_1 \max (0,t-\theta))
\]

Et donc, 
\begin{equation}\label{eq:esperance-TK-kingman-expansion}
    \mathbb{E}(T_K) = \int_0^\infty \mathbb{P} (T_K >t)dt 
    = \frac{1}{c_0}(1-e^{-c_0 \theta}) + \frac{e^{-c_0 \theta}}{c_1}
\end{equation}

Ensuite dans la précédente section, \autoref{sec:une-foret-pas-si-grande},
nous avons introduit la loi Beta$(2-\alpha,\alpha)$.
En partant des mêmes hypothèses que le \autoref{thm:pitman-sagitov}
et supposons que notre population % introduire le lien avec queue lourde, c etait mieux expliqué avant cette queue lourde avant... mais bon
a une reproduction à queue lourde, c'est-à-dire qu'un individu 
peut être à l'origine d'une partie non négligeable de la population. % citation nécessaire ! car pourquoi une queue lourde ferait ça et pas autre chose? déduit de l article de niwa Voila!
% Cette dernière 
% hypothèse représente le fait que certains individus
% peuvent avoir un nombre de descendants très élevé
% comparé à la moyenne de la population % citation ?. + c'est pas lié à une moyenne finie et variance infinie ?
Alors d'après \cite{Schweinsberg2003},
le processus converge vers un $\Lambda_0^\alpha$-coalescent % converge N->infty : n<<N et N c'est la population entiere
dont la mesure $\Lambda^\alpha_0$ est la loi
Beta$(2-\alpha,\alpha)$ avec $1<\alpha<2$. Nous convergeons vers une 
mesure de probabilité donc en notant $T_K^\alpha$
le temps de coalescence entre deux lignées sous ce modèle,
 $T_K^\alpha \sim \Exp(r_{2,2}) = \Exp(1)$ donc,
\begin{equation}\label{eq:esperance-TK-beta}
    \mathbb{E}(T_K^\alpha) = 1
\end{equation}

A présent si l'on souhaite utiliser un $\Lambda_0^\alpha$-coalescent
qui possède une certaine moyenne de discordance entre les lignées, 
$d_* \coloneq \mathbb{E}(K_{ij})$,
nous pouvons relier les paramètres des deux modèles par
\eqref{eq:esperance-TK}, \eqref{eq:esperance-TK-kingman-expansion} et 
\eqref{eq:esperance-TK-beta}.
\begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_Kij_overlap_retry_final}
        \caption{Histogrammes
        des discordances par paires $K_{ij}$ obtenues
        à partir de $n=200$ lignées simulées
        sous un $\Lambda_0^\alpha$-coalescent
        (en rouge, $\alpha = 1.33$) et sous un coalescent de Kingman à deux époques
        (en bleu, $(c_0, c_1, \theta) = (0.0141,0.306,7.2)$).
        Ces paramètres ont été choisis pour minimiser la distance en variation totale
        entre l'histogramme considéré et celui du $\Lambda_0^\alpha$-coalescent.
        Dans les deux modèles, le taux de mutation $\mu$
        est choisi de sorte que la moyenne des discordances soit fixée à
        $d_*=10$. 
        Les histogrammes sont construits à
        partir de $120$ réalisations
        indépendantes et montrent que les deux modèles produisent des
        distributions de discordances pratiquement indiscernables.}
        \label{fig:Kij_overlap}
    \end{figure}

    Dans la \autoref{fig:Kij_overlap}, nous avons deux modèles 
    biologiques plausibles pouvant expliquer la discordance d'une population.
    Ainsi connaître $(K_{ij})_{i\neq j}$
     ne permet pas d'identifier la mesure $\Lambda$.
    % En d'autres termes, avoir un modèle qui modélise parfaitement 
    % le nombre de discordance n'est pas suffisant pour définir un modèle.

% Visuellement, d'après
% la \autoref{fig:tmrca_foret}, pour
% $0<\alpha\leq 1$, la densité a plus de masse vers $1$ que $0$
% donc les fusions multiples sont bien trop probables.

 % on est d'accord
% la densité a aucun rapport avec cette queue lourde, faudrait plutto justifier autrement
% en disant que la masse est plsu proche de 0 que de 1 donc moyenne (de quoi ?) finie et variance infinie
% et que les superproducteur sont rares

Toutefois, si on fixe le taux de mutation $\mu$
pour les deux modèles alors à partir de données,
nous pouvons déterminer les paramètres de nos modèles et
regarder leur performance. 
Pour l'expérience numérique nous prenons $\alpha=1.3$ comme dans
\cite{Niwa2016} qui a été choisi comme le paramètre 
maximisant la vraisemblance. 
Pour le modèle de Kingman avec expansion, ils considèrent
un modèle à deux époques et estiment les paramètres démographiques
$(u_0,u_1)$, ce sont les tailles
effectives actuelles et ancestrales en unités de mutation. L'ajustement
par maximum de vraisemblance basé sur la distribution des différences
par paires donne $u_0 = 1$, $u_1 = 1.25$ et $\theta = 0.45$ \cite{Niwa2016}.
Dans notre paramétrisation en termes de taux de coalescence,
cela correspond à
\[
(c_0,c_1,\theta) = \Bigl(\frac{1}{u_0},\frac{1}{u_1},\theta\Bigr)
= (1,0.8,0.45)
\]


% lutôt que de choisir arbitrairement les paramètres de ce dernier, nous reprenons
% les estimations de \cite{Niwa2016} pour le modèle à deux époques ajusté aux mêmes
% données. Dans leur notation, $(u_0,u_1,t)$ désignent les tailles efficaces
% actuelle et ancestrale et la date de l'expansion, en unités de temps mutationnel,
% et le taux de coalescence entre deux lignées vaut $1/u_0$ avant $t$ et $1/u_1$ après
% $t$. En termes de notre fonction d'intensité $c$, cela revient à poser
% \[
% c_0 = \frac{1}{u_0},\qquad c_1 = \frac{1}{u_1},\qquad \theta = t,
% \]
% ce qui donne numériquement $(c_0,c_1,\theta) \approx (1{,}0,0{,}8,0{,}45)$ pour les
% valeurs de maximum de vraisemblance. La figure~\ref{fig:XXX} compare alors la
% distribution empirique de $K_{ij}$ aux bandes prédictives générées par ces deux
% modèles calibrés.
% \textcolor{red}{AUCUNE IDEE DE QUELS PARAMETRES ON A PRIS ET POURQUOI : voir 
% le code + demander à chatGPT}

%  afin que la distribution empirique
% des discordances par paires corresponde au mieux. % c'est quoi la vraisemlance mathématiquement ? qu'ont-ils fait comme calcul ?
% Puis % expliquer le c0, c1, theta pris dans l'expérience numérique et 
% comment ils ont été pris 
% J'ai absolument pas parlé de "d" , je ne sais pas ce que c'est, 
% à quel point c'est utile etc...

  \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_sardine_reel_val}
        \caption{Distribution empirique des discordances par paires chez la sardine japonaise
    (points noirs) comparée aux prédictions des deux modèles considérés.
    La courbe rouge correspond au $\Lambda_0^\alpha$-coalescent de loi
    $\mathrm{Beta}(2-\alpha,\alpha)$ avec $\alpha = 1.3$, et un intervalle
    de confiance à $95\%$ en rose obtenu par Monte Carlo ($300$ simulations).
    Le taux de mutation $\mu$ est calibré de sorte que la moyenne théorique
    des discordances sous ce modèle coïncide avec la moyenne empirique.
    La courbe bleue correspond au coalescent de Kingman à deux époques,
    avec $(c_0,c_1,\theta) = (1.00,0.80,0.45)$. La bande bleue
    indique l'intervalle de confiance à $95\%$.} %ARTICLE NIWA ILS PRENNENT  tau de mutation = 3 comme j'ai mis dans mon code
    \label{fig:sardine_reel_val}
    \end{figure}


    On remarque alors que le modèle de Kingman, même avec expansion, 
    ne permet pas d'expliquer la dynamique de ces sardines. Nous avons
    besoin d'utiliser un modèle permettant la coalescence de 
    multiples lignées à la fois, la mesure $\Lambda_0^\alpha$
    le permet d'une bonne manière et naturellement.



\section{Conclusion}

Dans ce rapport, nous avons étudié le $\Lambda$-coalescent,
en particulier le comportement du TMRCA
pour différentes mesures $\Lambda$ et une application aux
discordances par paires chez la sardine japonaise.
Nos simulations montrent que certains $\Lambda$-coalescents à
coalescences multiples
rendent mieux compte des données que le modèle de Kingman avec expansion.

Nous aurions aimé étudier plus en profondeur pour $n$ fixé et $n\to \infty$ 
des questions sur le TMRCA et la vitesse de convergence, un aspect qu'on a
dû délaisser par manque de temps et de place.
De plus une autre piste intéressante est d'utiliser un $\Xi$-coalescent,
un modèle plus général permettant la coalescence simultanée.

\newpage

% \textcolor{red}{Je mets en vrac, on verra plus tard}
% On aurait aimé s'appuyer un peu plus sur l'étude pour $n$ fixé sur le TMRCA.
% Etudier le modèle asymptotique à l'instar de "silence ça pousse".
% Etudier la vitesse de convergence.
% Généraliser par le $\Xi$-coalescent, permettant à un même instant
% de coalescer différentes lignées mais pas forcément
%  ensemble comme actuellement (citer le pdf associé).
%  Approfondir les mutations et développé le modèle en s'axant plus vers 
% des considérations biologiques.
% Dans la derniere section on a considérer 2 époques pour simplifier 
% l'analyse mais il s'agirait de développer plus le modèle,
% de poser plus formellement les limites biologiques et donc mathématiques, blabla.
% Etre plus précis sur certaines remarques.
% Par exemple, calculer l'écart 
% des histogrammes pour savoir à quel point ils sont semblables ou non.



\bibliographystyle{alpha}
\bibliography{ref}
\end{document}