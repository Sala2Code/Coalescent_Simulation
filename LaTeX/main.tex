\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{natbib}
\usepackage{hyperref} % garder en dernier
\usepackage{svg}
\usepackage{float}

\svgsetup{
  inkscapelatex=true,
  inkscapeexe=inkscape,
  clean=true
}
% Set a default search path for svg figures (so \includesvg{plot_silence_ca_pousse} works)
\svgpath{{photo/}}
% Autoref names (français) — évite les warnings de hyperref lorsque
% on utilise \autoref sur des environnements non-standards
\providecommand{\theoremautorefname}{Théorème}
\providecommand{\lemmaautorefname}{Lemme}
\providecommand{\propositionautorefname}{Proposition}
\providecommand{\definitionautorefname}{Définition}
\providecommand{\sectionautorefname}{Section}
\providecommand{\subsectionautorefname}{Sous-section}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{bbm}

\geometry{
    left=1.cm,
    right=1.cm,
    top=1.5cm,
    bottom=1.5cm
}

\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}

\newcommand{\Exp}{\mathrm{Exp}}

\title{Analyse du $\Lambda$-coalescent : renouer avec ses racines. }
\author{SALA Raphaël, MUGISHA Axcel, GARCIA Hugo, COLLIN Thibault}
\makeatletter
\newcommand{\monthandyear}{%
    \ifcase\month\or janvier\or février\or mars\or avril\or mai\or juin\or juillet\or août\or septembre\or octobre\or Novembre\or Décembre\fi\ \number\year}
\makeatother
\date{\monthandyear}




\begin{document}

\maketitle

    \section{Introduction}
    \subsection{Fondements du $\Lambda$-coalescent}

    La théorie de la coalescence modélise le phénomène par lequel des
    individus d'une population partagent un ancêtre commun. Nous souhaitons 
    étudier rétrospectivement leur évolution.

    Historiquement,
    le modèle de Wright-Fisher étudie une population de taille finie $N$
    où les individus d'une générations coalescent de manière uniforme entre 
    eux dans la génération précédente \cite{Fisher1930}.
    Ensuite, le modèle de Kingman \cite{Kingman1982} est le modèle
    limite de Wright-Fisher où l'on s'intéresse à $n<N$ lignées
    et en considérant $N\to +\infty$.
    Ce cadre asymptotique permet de
    simplifier grandement l'étude du phénomène de coalescence. 
    Le modèle peut à présent être décrit comme un processus de Markov.

    En 1999, Pitman et Sagitov
    généralisent le modèle de Kingman en autorisant la coalescence
    simultanée de plusieurs lignées. Des individus peuvent engendrer 
    une proportion non négligeable de la population. 
    Afin de définir un modèle, nous supposons raisonnablement
    que les lignées coalescent aléatoirement et indépendamment 
    de leur histoire passée,
    c'est-à-dire en supposant l'absence de mémoire (propriété de Markov),
    que toutes les lignées ont les mêmes chances de coalescer entre elles
    que l'on appelle l'échangeabilité et enfin que nous ayons 
    l'absence de collisions multiples signifiant qu'à tout instant donné,
    il ne peut y avoir qu'un seul événement de fusion en un même ancêtre.


    \begin{theorem}[Pitman-Sagitov {\cite{Pitman1999,Sagitov1999}}]
    Il existe un processus de Markov, $(N_t)_{t\geq 0}$, appelé $\Lambda$-coalescent,
    échangeable à collisions multiples simples si et seulement s'il
    existe une mesure finie $\Lambda$ sur $[0,1]$ telle que, lorsqu'on a $b$ lignées,
    pour tout $2\le k\le b$ le taux auquel chaque $k$-uplet fixé de lignées
    fusionne vaut, 
    \[
    \lambda_{b,k}=\int_0^1 x^{k-2}(1-x)^{b-k}\,\Lambda(dx)
    \]
    \end{theorem}

    Nous ne définissons par formellement les conditions ici et donnons
    encore moins une preuve car cela est au-delà du cadre de ce rapport.
    Ce résultat montre que la dynamique de $(N_t)_{t\geq 0}$,
    indiquant le nombre de lignées à l'instant $t$, est entièrement caractérisée
    par une mesure finie. Sans perte de généralité nous considérons pour la suite
    une mesure de probabilité, $\Lambda$ sur $[0,1]$. 
    Partant de $b$ lignées,
    le taux d'une $k$-coalescence ($2\leq k \leq b$) est 
    $r_{b,k} \coloneq \binom{b}{k} \lambda_{b,k}$. 
    Le taux de sortie de l'état $b$ est la somme des taux donc
    
    \begin{equation} \label{eq:lambda_b}
        \lambda_b = \sum_{k=2}^{b} r_{b,k} = \int_0^1 S_b(x) \,\Lambda(dx),
        \quad
        S_b(x) \coloneq \sum_{k=2}^{b} \binom{b}{k} x^{k-2}(1-x)^{b-k}
        = \frac{1-(1-x)^b - b x (1-x)^{b-1}}{x^2}
    \end{equation}
    
    D'après le lemme des réveils, à chaque événement de coalescence on passe de $b$ à $b-k+1$ lignées
    avec probabilité, 
    \[
    \forall b\geq k \geq 2,
    \quad p_{b,k} \coloneq \frac{r_{b,k}}{\sum_{k=2}^b r_{b,k}} = \frac{\binom{b}{k} \lambda_{b,k}}{\lambda_b}
    \]
    Ainsi, le squelette du processus est une chaîne de Markov
    décroissante sur $\llbracket 1, n \rrbracket$, commençant en $n$ et
    absorbée presque sûrement en $1$.

    \subsection{Exemple (Kingman)} \label{sec:kingman-example}
        Intéressons-nous au modèle de Kingman en guise d'introduction. 
    On pose $\Lambda = \delta_0$.
    Pour $2\leq k\leq b $,
    \[
    \lambda_{b,k}
        = \int_0^1 x^{k-2}(1-x)^{b-k} \delta_0(x) dx
        = [x^{k-2}(1-x)^{b-k}]_{x=0} 
        =
    \begin{cases}
    (1-0)^{b-2} = 1 & \text{si } k = 2 \\
    0^{k-2}(1-0)^{b-k} = 0 & \text{si } k > 2
    \end{cases}
    \]

    Les coalescences se font que par paires. Une caractéristique intéressante
    du $\Lambda$-coalescent
    est le TMRCA (Time to the Most Recent Common Ancestor),
    c'est-à-dire le plus petit temps tel que toutes les lignées
    ont fusionné en un ancêtre commun. Dans la suite du rapport, nous le notons
    \[
    \tau_{\Lambda,n} = \inf \{ t\geq 0, N_t=1, N_0=n \}
    \]
    Lorsque le contexte est clair sur $\Lambda$ ou $n$, ceux-ci seront omis 
    afin de rendre la lecture plus agréable.
    
    \begin{lemma}\label{lem:recurrence-TMRCA}
        % Notons le TMRCA d'un $\Lambda$-coalescent $\tau\coloneq \inf \{ t\geq 0, N_t=1 \}$.
        Soit $\Lambda$ une mesure de probabilité sur $[0,1]$. Notons
        $H: b \in \mathbb N^* \longmapsto \mathbb{E}_\Lambda(\tau \mid N_0 = b)$.
        Alors $H(1)=0$, $H(2)=1$ et pour $b\geq 3$,
        \[
        H(b) = \frac{1}{\lambda_b} \;+\; \sum_{k=2}^{b-1} p_{b,k}\, H(b-k+1)
        \]
    \end{lemma}

    \begin{proof}
    Pour $b=1$, $N_t=1$  donc $H(1)=0$.
    Pour $b=2$, le seul saut possible est de $2$ vers $1$ lignée
    avec taux $\lambda_2=\binom{2}{2}\lambda_{2,2}=1$, d'où $\tau\sim \Exp(1)$ et $H(2)=1/1=1$.

    Fixons $b\ge 3$. Définissons le temps de la première coalescence,
    \[
    T_1:=\inf\{t\geq0, N_t\neq b\}
    \]

    $(N_t)_{t\geq 0}$ est un processus de Markov avec un taux de saut $\lambda_b$,
    donc $T_1\sim \Exp(\lambda_b)$
    et donc $\mathbb{E}_\Lambda(T_1 \mid N_0 = b)=\frac{1}{\lambda_b}$.
    De plus, si $K$ est la taille de la fusion au temps $T_1$, alors 
    $
    K \sim \sum_{k=2}^{b} p_{b,k} \delta_k
    $
    et $N_{T_1}=b-K+1$.

    Considérons la filtration naturelle $(\mathcal{F}_t)_{t\geq 0}$
    de $(N_t)_{t\geq 0}$. Par la propriété de Markov forte au temps $T_1$ et
    l'absence de mémoire,
    \[
        \mathbb E_\Lambda(\tau-T_1\mid \mathcal F_{T_1}, N_0=b )
        =\mathbb E_\Lambda(\tau \mid N_{T_1})
        =H(N_{T_1})
    \]
    
    Ainsi en conditionnant par $\mathcal{F}_{T_1}$,
    \begin{align*}
        H(b)
        &=\mathbb E_\Lambda(\tau \mid N_0=b)
        =\mathbb E_\Lambda(T_1 \mid N_0=b)+\mathbb E_\Lambda(\tau-T_1 \mid N_0=b)
        % &=\mathbb E_b[T_1]+\mathbb E_b\!\big[\mathbb E_b[\tau-T_1\mid \mathcal F_{T_1}]\big]\\
        =\frac{1}{\lambda_b}+\mathbb E_\Lambda(H(N_{T_1}) \mid N_0=b)\\
        &=\frac{1}{\lambda_b}+\sum_{k=2}^{b}\mathbb P_b\big(N_{T_1}=b-k+1\big) H(b-k+1)
        =\frac{1}{\lambda_b}+\sum_{k=2}^{b} p_{b,k} H(b-k+1)
    \end{align*}

    Or $H(1)=0$, donc le terme $k=b$ s'annule. D'où le résultat.
    \end{proof}

    
    
    
    Pour $b$ lignées observées, on a
    $ \lambda_b = \sum_{k=2}^{b} \binom{b}{k} \lambda_{b,k}
               = \binom{b}{2} \lambda_{b,2}
               = \binom{b}{2}
        $
    donc, d'après le \autoref{lem:recurrence-TMRCA}, la taille moyenne d'un arbre pour
    le modèle de Kingman est donné par,
    \[
    H(b) = \frac{1}{\lambda_b} + p_{b,2}\,H(b-1)
    = \frac{1}{\binom{b}{2}} + H(b-1)
    \]

    Ainsi par récurrence,
    \begin{equation}\label{eq:tmrca-kingman}
        H(b)=\sum_{k=2}^{b}\frac{1}{\binom{k}{2}}
        = \sum_{k=2}^{b}\frac{2}{k(k-1)}
        = \sum_{k=2}^{b}2\Big(\frac{1}{k-1}-\frac{1}{k}\Big)
        =2\Big(1-\frac{1}{b}\Big)
    \end{equation}
    
    
    \textcolor{red}{
        Là je propose donc (1 page grand max)
        (le but n'est pas de faire une étude de Kingman mais de présenter les différents 
        objets du modèle de manière simple et visuelles)
        \begin{itemize}
            \item  poser $\Lambda = \delta_0$, 
            \item l'intuition du modèle : la masse est vers 0 donc
            pas de grosse fusion. (Inteprétation du modele, du role des termes de l'intégrande)
            \item Les calculs en 5 lignes maximum ($\lambda_{b,k}$, TMRCA si utile, $\dots$ )
            \item Les notations : $n$, $N_t$, $(C_t^i)_{0<i<n}$, TMRCA, $T_k$
        \end{itemize}
    }
    
%\textcolor{red}{
%    Subplots (kingman)
%    \begin{itemize}
%        \item (Une réalisation) Arbre + TMRCA 
%        \item (Sur plusieurs réalisaiton) Distribution des fusions ((C\_t\^{}i))\_\{0<i<n \} (donc uniquement en 2 normalement)
%        \item  (Sur plusieurs réalisation) distribution TMRCA + densité avec en légende "densité théorique"
%        dans le caption mettre, ou en footnote, que la densité est explcite dans ce cas dont la formule 
%        n'est pas prouvée, ele peut être écritre matriciellement d'après (citer "une foret pas si grande")
%    \end{itemize}
%}

    \section{Analyse du TMRCA}
    \subsection{Aux extrêmes de l'arbre. }

    Au vu du précédent exemple, on peut se demander l'influence de la mesure
    $\Lambda$ sur le TMRCA. Intuitivement, ce temps moyen devrait diminuer lorsque
    la masse de $\Lambda$ se rapproche de 1 puisqu'on autorise des coalescences
    multiples plus importantes. En première analyse on va étudier
    les deux cas extrêmes.

    % \begin{lemma}\label{lem:bounds-Sb}
    %     Pour tout $b\geq 2$ et $x\in[0,1]$,
    %     \[
    %     1 \leq S_b(x) \leq \binom{b}{2}
    %     \]
    % \end{lemma}
    % \begin{proof}
    %     Soit $(X_i)_{1\geq i \geq k}$
    %     de loi de Bernouilli de paramètre $x \in [0,1]$ indépendantes.

    %     Posons $X\coloneq\sum_{i=1}^b X_i \sim \mathrm{Bin}(b,x)$. On a,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \mathbb{E}(\sum_{i\neq j} X_i X_j)
    %     \overset{\perp\!\!\!\perp}{=}\sum_{i\neq j} \mathbb{E}(X_i)\mathbb{E}(X_j)  
    %     = b(b-1)x^2
    %     \]

    %     En utilisant que $k(k-1) \geq 2 \mathbbm{1}_{k\geq 2}$,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \sum_{k=0}^b k(k-1) \binom{b}{k} x^k (1-x)^{b-k}
    %     \geq \sum_{k=2}^b 2 \binom{b}{k} x^k (1-x)^{b-k}
    %     = 2 S_b(x)x^2
    %     \]

    %     D'où,
    %     \[
    %     2S_b(x)x^2 \leq b(b-1)x^2 
    %     \Longleftrightarrow
    %     S_b(x) \leq \frac{b(b-1)}{2} = \binom{b}{2}
    %     \]

    %     Pour l'autre inégalité, on remarque que pour tout $x\in]0,1]$,
    %     \[
    %     S_b(x) = \frac{1}{x^2}\mathbb P(X\geq 2)
    %     \geq \frac{1}{x^2} \mathbb{P}(X_1=X_2=1) = \frac{x^2}{x^2} = 1
    %     \]

    %     Puis $S_b(0) = \binom{b}{2} \geq 1$, d'où le résultat.
    % \end{proof}

    \begin{proposition}
        Soit $n$ le nombre de lignées. Notons le TMRCA d'un $\Lambda$-coalescent,
        \[
        \tau\coloneq \inf \{ t\geq 0, N_t=1 \}
        \]

        Alors, pour toute mesure de probabilité $\Lambda$ sur $[0,1]$, on a
        condtionnellement à $\{N_0 = n\}$,
        \[
        1 = \mathbb{E}_{\delta_1}(\tau) 
        \leq \mathbb{E}_{\Lambda}(\tau)
        % \leq\mathbb{E}_{\delta_0}(\tau) = 2\left(1-\frac{1}{n}\right)
        \]
    \end{proposition}
    \begin{proof}
    % L'égalité de droite a été montrée dans l'exemple de Kingman (voir \autoref{sec:kingman-example}).
    Prouvons l'égalité. Prenons $\Lambda = \delta_1$, nous avons $\lambda_{n,k} = \delta_{n,k}$ (symbole de Kronecker),
    donc $\lambda_n = \binom{n}{n} \lambda_{n,n} = 1$ donc
    $\tau \sim \Exp(1)$ et donc $\mathbb{E}_{\delta_1}(\tau) = 1/1 = 1$.
    
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$.
    Notons $H(b) \coloneq \mathbb{E}_\Lambda(\tau \mid N_0 = b)$.
    % D'après \eqref{eq:lambda_b}, 
    % D'après la propriété de Markov et l'absence de mémoire de l'exponentielle,
    % \[
    % H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    % \]
    
    Montrons par récurrence forte l'inégalité, c'est-à-dire $H(b)\geq 1$
    pour $b\geq 2$.
    % Pour $b=2$, $\lambda_{2,2}=1$ donc $\tau \sim \Exp(1)$
    % donc $H(2) = 1 \geq 1$. 
    L'initialisation a été prouvée dans le lemme \ref{lem:recurrence-TMRCA}.
    Supposons l'inégalité vraie jusqu'à $b-1$. 
    Remarquons que $\lambda_{b,b} = \int_0^1 x^{b-2}\,\Lambda(dx) \leq \int_0^1 \Lambda(dx) = 1$,
    \[
    H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    \geq \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} 
    = \frac{1}{\lambda_b} + 1 - p_{b,b}
    = 1 + \frac{1 - \lambda_{b,b}}{\lambda_b} \geq 1
    \]

    D'où le résultat.
    \end{proof}

    Cette idée de déplacer la masse de $\Lambda$ vers 1 pour diminuer la moyenne 
    du TMRCA
    est intuitive. Pour le problème inverse de maximisation du TMRCA nous
    souhaiterions déplacer la masse de $\Lambda$ vers 0. C'est-à-dire prouver que 
    le modèle de Kingman soit celui maximisant le temps moyen du TMRCA. 
    Toutefois, voila une grande surprise : ce n'est pas le cas !

    \begin{proposition}\label{prop:surprise-TMRCA}
        Il existe $n>1$ et une mesure de probabilité $\Lambda$ sur $[0,1]$
        telle que, conditionnellement à $\{N_0=n\}$,
        \[
        \mathbb{E}_{\Lambda}(\tau)> \mathbb{E}_{\delta_0}(\tau)
        \]
    \end{proposition}
    \begin{proof}
        Soit $n=8$, dans l'exemple de Kingman (voir \autoref{sec:kingman-example}),
        nous avons une formule explicite.
        \[\
        \mathbb E_{\delta_0}(\tau) = 2\left(1-\frac{1}{8}\right) = \frac{14}{8} = 1.75
        \]

        Soit $\Lambda = \delta_{1/4}$, alors d'après \eqref{eq:lambda_b},
        \[ 
        \lambda_{n,k} = \left(\frac{1}{4}\right)^{k-2} \left(\frac{3}{4}\right)^{n-k}
        \quad 
        \lambda_n = 16\left(1 -\left(\frac34\right)^n - \frac{n}{4}\left(\frac34\right)^{n-1}\right)
        \] 
        
        Ainsi, en calculant nous obtenons,
        \[ 
        \mathbb{E}_{\delta_{1/4}}(\tau)
        = \frac{1}{\lambda_n} + \sum_{k=2}^{n-1} \frac{\binom{n}{k} \lambda_{n,k}}{\lambda_n} \mathbb{E}_{\delta_{1/4}}(\tau \mid N_0 = n-k+1)
        = \frac{19954284839411683}{11337879079537330} > 1.7599662 \dotsc > 1.75
        \]
    \end{proof}
    Nous conjecturons que le théorème peut être étendu pour tout $n>6$.
    A notre connaissance l'étude ce phénomène n'est pas documenté pour $n$ fini.
    Seul un article de \cite{kluge2017exchangeable} s'intéresse
    la croissance de $ \sup_\Lambda \mathbb{E}_\Lambda(\tau)$ lorsque $n\to\infty$.


      \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_tmrca_extrema_histograms}
        \caption{Distribution empirique du TMRCA pour $n=8$
        de $\Lambda=\delta_0$ (Kingman, histogramme vers le haut en bleu)
        et $\Lambda=\delta_{1/4}$ (histogramme vers le bas en orange).
        Les bandes verticales pointillées délimitent les intervalles
        de confiance (IC) à $95\%$ pour la moyenne de
        $\tau$ basés sur $M=1e5$ simulations
        indépendantes. Les lignes pleines indiquent les moyennes
        empiriques et les lignes pointillées ("-.")
        les moyennes théoriques.}
        \label{fig:tmrca-extrema}
    \end{figure}

    Dans la \autoref{fig:tmrca-extrema} nous utilisons des intervalles
    de confiances.
    Soient $(T_i)_{1\leq i \leq M}$ collections de variables 
    aléatoires i.i.d. de loi $\tau_\Lambda$. Posons la moyenne et
    l'estimateur de la variance
    \[
        \overline T_M \coloneq \frac{1}{M}\sum_{i=1}^M T_i\qquad
        s_M^2 \coloneq \frac{1}{M-1}\sum_{i=1}^M (T_i - \overline T_M)^2
    \]
    
    Les intervalles de confiances sont construits génériquement.
    D'après le théorème central limite et le lemme de Slutsky,
    \[
        \sqrt{M}\frac{\overline T_M -
        \mathbb{E}_\Lambda[\tau]}{\sqrt{s^2_M}}
        \xrightarrow[M\to\infty]{\mathcal{L}} \mathcal N(0,1)
    \]

    Ainsi, un IC asymptotique de niveau $1-\alpha$ est
    \[
        [ \overline T_M \pm q_{1-\alpha/2}
        \frac{\sqrt{s^2_M}}{\sqrt{M}}]
    \]

    Dans la \autoref{fig:tmrca-extrema} 
    le zoom à droite montre les intervalles de confiance à $95\%$ disjoints
    renforçant l'observation $\mathbb E(\tau_{\delta_0, 8}) \leq
    \mathbb E(\tau_{\delta_{1/4}, 8})  $.


   \textcolor{blue}{L'échelle de temps ici est en unités de $N$ générations, avec $N\gg n$ puisque
    nous considérons un modèle asymptotique.}

    % TODO : Reformuler !
    % Une suprise ... À notre connaissance,
    % la question de déterminer, pour un (n) fixé,
    % la mesure (\Lambda) qui maximise l’espérance du temps
    % d’absorption (\mathbb E_\Lambda[\tau]) du (\Lambda)-coalescent
    % reste ouverte. Les travaux existants se concentrent sur les lois
    % limites et les constantes asymptotiques de (\tau_n) lorsque
    % (n\to\infty) (voir p.ex. Kersting–Wakolbinger), sans adresser
    % l’extrémalité en (\Lambda) pour (n) fini.


    \subsection{Une forêt pas si grande }

    Un processus de Markov est entièrement déterminé
    par son générateur infinitésimal.
    Pour $n$ lignées observées,
    celui d'un $\Lambda$-coalescent
    est la matrice triangulaire inférieure
    $Q \in \mathcal M_n(\mathbb{R})$ définie 
    pour tout $1\leq b,i\leq n$, par
    \[
        Q_{b,i} =
        \begin{cases}
        r_{b,k} & \text{si } b\ge 2 \text{ et } i = b-k+1 \text{ pour } 2\le k\le b\\
        -\lambda_b & \text{si } b\ge 2 \text{ et } i = b\\
        0 & \text{sinon}
        \end{cases}
    \]

    Le premier élément de sa diagonale, $Q_{1,1}$, est nul
    car l'état $1$ est absorbant donc $Q$ n'est pas inversible.
    En se restreignant à la sous-matrice $R = (Q_{i,j})_{2\leq i,j \leq n}$
    la matrice devient inversible et nous pouvons exprimer la densité de $\tau$.
    Posons $p_R(t) = (p_k(t))_{2\leq k \leq n}$
    où $p_k : t \geq 0 \mapsto \mathbb{P}(N_t = k \mid N_0=n)$. D'après
    la relation de Chapman-Kolmogorov, $p_R$ vérifie pour tout $t\geq 0$
    \[
    \begin{cases}
        p_R'(t) = p_R(t) R \\
        p_R(0) = (0,\dots,0,1)
    \end{cases}
    \iff p_R(t) = (0,\dots,0,1) e^{tR}
    \]
    Définissons la fonction de survie,
    $S : t \mapsto \mathbb{P}(\tau_n > t) = \mathbb{P}(N_t\neq 1 \mid N_0=n)
    = \sum_{k=2}^n p_k(t) = p_R(t) \cdot \textbf{1}$.
    Donc la densité de $\tau_n$ est donnée par, 
    \begin{equation}\label{eq:density-TMRCA}
        f_\tau : t\mapsto d_t (1-S(t))
        = -S'(t) = -p_R'(t) \cdot \textbf{1}
        = -p_R(t) R \cdot \textbf{1}
        = - (0,\dots,0,1) e^{tR} R \cdot \textbf{1}
    \end{equation}
   


    On remarque également que ce processus est défini 
    par $(\lambda_{b,k})_{I_n}$ avec
    $I_n \coloneq \{(b,k), 2\leq k \leq b \leq n \}$.
    Définissons pour $r \in \llbracket 0, n-2 \rrbracket$
    \[ 
    m_r : \Lambda \mapsto \int_0^1 x^r \Lambda(dx)
    \]

    En développant l'intégrande des taux de fusions, 
    pour tout $(b,k) \in I_n$,
    il existe $A_n \in \mathcal M_{I_n, n-1}(\mathbb{R})$ tel que, 
    \[
    \lambda_{b,k} = \sum_{r=0}^{n-2} A_{(b,k),r} m_r(\Lambda)
    \]
        
    Pour $n$ fixé, $Q$ est entièrement déterminée
    par $(m_r(\Lambda))_{0\leq r \leq n-2}$,
    l'espace
    des mesures de probabilité sur $[0,1]$ se réduit
    à une projection
    de dimension finie, $\mathbb{R}^{n-1}$, donc un espace bien plus petit.
    Autrement dit, 
    une infinité de mesures différentes
    deviennent indiscernables pour un processus considéré.
 
    \textcolor{red}{Est-ce possible de modifier les expressions mathématiues pour pas 
    que ça soit chatGPT ? les ., les $()$ au lieu de $][$, etc... }
\begin{proposition}\label{prop:beta_indiscernable}
    Soit $n>1$ et $\Lambda_0=\mathrm{Beta}(2-\alpha,\alpha)$ avec $\alpha \in ]1,2[$,
    \textcolor{red}{Ca depend de $\alpha$ mais on l'appelle $\Lambda_0$, etrange... 
    Peut-être $\Lambda_0^\alpha$. Je précise que j'avais fait la notation 
    $\Lambda_\varepsilon$ et $\Lambda_0$ car si tu prenais $\varepsilon=0$, cela 
    correspondait bien. Donc si possible juste mettre des indices n'importe où
    et trouver une cohérence.}
    de densité \[
    w_\alpha : x\in [0,1] \longmapsto
    \frac{1}{B(2-\alpha,\alpha)} x^{1-\alpha}(1-x)^{\alpha-1}
    \]

    Soit $Q_{k}$ \textcolor{red}{Q est pris pour la matrice de générateur infinitésimal} un polynôme de Jacobi \textcolor{red}{expression des polynomes ?} de degré $k$,
    orthogonal à tous les polynômes de degré inférieur à $n-1$ pour
    le produit scalaire  \textcolor{red}{une citation est nécessaire}
    \[
    \langle f,g\rangle_\alpha=\int_0^1 f(x)g(x) w_\alpha(x)\,dx 
    \]

      On pose
  \[
    M:=\sup_{x\in[0,1]}|Q_{n-1}(x)| \in (0,+\infty)
    \qquad\text{et}\qquad
    \varepsilon_0 := \frac{1}{M}.
  \]
  Pour $\varepsilon\in(0,\varepsilon_0)$, on définit la mesure de probabilité $\Lambda_{\varepsilon} \neq 
    \Lambda_0$ sur $[0,1]$ par sa densité
    
    \[
    f_\varepsilon(x)
    =\bigl(1+\varepsilon Q_{n-1}(x)\bigr)\,w_\alpha(x),
    \qquad x\in[0,1],
    \]

    Alors, pour tout $0<\varepsilon<\varepsilon_0$,
    \[
    \tau_{\Lambda_\varepsilon,n}
    \overset{\mathcal{L}}{=}
    \tau_{\Lambda_0,n}
    \]
\end{proposition}
\begin{proof}
    Montrons que pour $\varepsilon$ assez petit
    \textcolor{red}{hmm.. pouruqoi pas expliciter les valeurs possibles?},
     $f_\varepsilon$ est bien une densité
    de probabilité. \textcolor{red}{Plutot mettre "Montrons que f est une densité", je 
    trouve ça plsu naturelle mais comme tu veux}
    
    En prenant $\varepsilon<1/M$, on a
    \[
        1+\varepsilon Q_{n-1}(x)\ge 1-\varepsilon M \ge 0,
    \]
    donc $f_\varepsilon\ge 0$ sur $[0,1]$.

    Par ailleurs, l’orthogonalité de $Q_{n-1}$ avec la constante $1$ donne
    \[
        \int_0^1 Q_{n-1}(x) w_\alpha(x)\,dx = 0,
    \]
    d’où
    \[
        \int_0^1 f_\varepsilon(x)\,dx
        = \int_0^1 w_\alpha(x)\,dx
          + \varepsilon \int_0^1 Q_{n-1}(x)w_\alpha(x)\,dx
        = 1 + \varepsilon\cdot 0 = 1.
    \]
    Ainsi $f_\varepsilon$ est bien une densité de probabilité sur $[0,1]$.

    Montrons à présent que les générateurs infinitésimaux de $\Lambda_0$ et
    $\Lambda_\varepsilon$ coïncident sur $\{1,\dots,n\}$.
    Soit $r \in \llbracket 0, n-2 \rrbracket$. Comme $x^r$ est un polynôme de degré
    $\le n-2$, l’orthogonalité de $Q_{n-1}$ implique
    \[
        \int_0^1 x^r Q_{n-1}(x) w_\alpha(x)\,dx = 0.
    \]
    Ainsi,
    \[
        m_r(\Lambda_\varepsilon)
        = \int_0^1 x^r f_\varepsilon(x)\,dx
        = \int_0^1 x^r w_\alpha(x)\,dx
          + \varepsilon \int_0^1 x^r Q_{n-1}(x) w_\alpha(x)\,dx
        = m_r(\Lambda_0).
    \]

   Les taux de fusions sont donc égaux entre ces mesures,
    d'où le résultat.
\end{proof}


%\begin{proposition}\label{}
%Soit $n>1$. Notons $P_k$ le polynôme de 
%Legendre de degré $k$. 
%Prenons $\Lambda_0$ la mesure uniforme 
%sur $[0,1]$. Pour tout $0<\varepsilon<1$,
%définissons la mesure de probabilité
%$\Lambda_\varepsilon \neq \Lambda_0$
%de densité sur $[0,1]$
%\[ 
%f_\varepsilon(x) = 1 + \varepsilon P_{n-1}(2x-1)
%\]
%
%Alors, pour tout $0<\varepsilon<1$, 
%\[
%\tau_{\Lambda_0} \overset{\mathcal{L}}{=} \tau_{\Lambda_\varepsilon}
%\]

%\end{proposition}
%\begin{proof}
%Montrons que pour tout $0<\varepsilon<1$,
%$f_\varepsilon$ est bien une densité de probabilité.
%Nous avons, par une analyse élémentaire que
%pour tout $k\geq 0$
%$|P_k|\leq 1$ sur $[-1,1]$
%\cite{WhittakerWatson1920} donc $f_\epsilon \geq 1-1\cdot\varepsilon \geq0 $.
%
%Rappelons que $(P_k)_{0\leq k \leq n}$ est une base orthogonale de 
%$\mathbb{R}_n[X]$ pour le produit scalaire 
%$f,g \mapsto \int_{-1}^1 f(x)g(x) dx$
%donc $\int_0^1 f_\varepsilon(x)dx =
%1+\frac{\varepsilon}{2} \int_{-1}^1 1\cdot P_n(x) dx = 1+0 = 1$. 
%Montrons à présent que le générateur infinitésimal
%de $\Lambda_0$ et $\Lambda_\varepsilon$ sont identiques.
%Soit $r \in \llbracket 0, n-2 \rrbracket$,
%puisque $X^r \in \mathbb{R}_{n-2}[X]
%\subset \mathbb{R}_{n-1}[X]$,
%\[
%m_r(\Lambda_\varepsilon)
%= \int_0^1 x^r f_\varepsilon(x) dx
%= \int_0^1 x^r dx + \varepsilon \int_0^1 x^r P_{n-1}(2x-1) dx
%= \frac{1}{r+1} + \varepsilon \cdot 0
%= m_r(\Lambda_0)
%\]
%
%Les taux de fusions sont donc égaux entre ces mesures,
%d'où le résultat.
%\end{proof}

    Ainsi nous venons de construire une infinité de mesures 
    différentes qui induisent le même processus de coalescence.
    On s'attendait à obtenir une infinité d'arbres généalogiques
    différents mais ceux-ci sont identiques en loi.

    \textcolor{red}{Une phrase pour expliquer tout ça? ou donner des idées.
    Car là on a 2 paramètres donc ça peut embrouiller l'esprit, qu'est ce qui 
    fait quoi, pourquoi avoir choisi alpha, 2-alpha alors que la loi beta est 
    sur alpha,beta>0 ; pourquoi avoir ajouté un epsilon alors qu'on a déjà un parametre
    qui bouge. Ca vaut la peine d'y consacrer un (petit) paragraphe. (Repond à 
    cette question sans ChatGPT) }

    % afficher iamge svg

        \textcolor{red}{C'est cool la théorie marche mais les 2 premiers subplots sont pas 
        si différents donc ils sont pas si convaicant. Prend des epsilon plus petit/grand
        pareil pour les alpha car le premier et le deuxieme c'est les memes à moins
        que t'aies une remarque à faire.}
        
        \textcolor{red}{Je préferais mon historgramme car la ca fait un seul histogramme 
        marron vu que ça se condond. A moins que t'appréciais pas le mien (c'est possible,
        on peut  discuter d'autres solution), donne mes codes à chatGPT pour qu'il te
        fasse un histogramme plein (orange par exemple + alpha) et l'autre uniquement
        sa bordure, c'est plus visuelle je trouve.
        Il manque les labels sur les axes. Le troiseme plot je n'apprecie pas le ylim.
        Il devrait être à (0.5,1.5) pour que ça soit centré. De plus, dans le caption 
        tu peux dire que cette mesure c'est la loi uniforme (ou dans le parapgrahe avant
        mais je pense c'est pertinent de dire que la loi beta représente pas mal de loi, 
        après avoir selon les alphas possibles car ça restreint pas mal tout de meme)}

        \textcolor{red}{dans les plots à droites, la legende devrait indiquer
        "histogramme empirique" mais pas TMRCA car c'est déjà indiqué dans le titre.}


    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_foret_pas_gde_beta}
        \caption{(a) \textit{Gauche}~: comparaison des densités remarquablement
        différentes
        sur $[0,1]$, \(f_\varepsilon(x)=\bigl(1+\varepsilon Q_{n-1}(x)\bigr)\,w_\alpha(x)\),
        pour $n=8$~: cas Beta($2-\alpha,\alpha$) $\varepsilon=0$ et perturbation de Jacobi $\varepsilon=0.5$
        (b) \textit{Droite}~: distribution empirique du TMRCA,
         issues de $n$ lignées, à partir
        de 8000 simulations, avec superposition de la densité théorique
        donnée par \eqref{eq:density-TMRCA}. Comme attendu les densités se
        confondent.}
    \end{figure}
%\begin{figure}[H]
%    \centering
%    \includesvg[width=\textwidth]{photo/plot_foret_pas_gde_beta}
%    \caption{(a) \textit{Gauche}~: comparaison des densités remarquablement
%    différentes
%    sur $[0,1]$, \(f_\varepsilon(x)=1+\varepsilon P_{n-1}(2x-1)\),
%    pour $n=10$~: cas uniforme
%    $\varepsilon=0$ et perturbation de Legendre $\varepsilon=0.5$
%    (b) \textit{Droite}~: distribution empirique du TMRCA,
%     issues de $n$ lignées, à partir
%    de 8000 simulations, avec superposition de la densité théorique
%    donnée par \eqref{eq:density-TMRCA}. Comme attendu les densités se
%    confondent.}
%\end{figure}

   

    \subsection{Silence, ça pousse}
    Précedemment nous avons brièvement parlé de la mesure uniforme en 
    prenant \textcolor{red}{si tu peux finir la phrase selon les notations prises}.
    Ce modèle est connu sous le nom de Bolthausen-Sznitman
    et décèle un résultat incontournable.
    % \begin{theorem}[C. Goldschmidt \& J. B. Martin \cite{goldschmidt-martin-2005}]
    \begin{theorem}[Goldschmidt \& Martin~\cite{goldschmidt-martin-2005}]
        Soit $(N_t)_{t\geq 0}$ un Bolthausen-Sznitman coalescent.
        % Pour $n>0$,
        % $ \tau_n \coloneq \inf\{t\geq 0, N_0 = n, N_t=1\}$. 
        Alors, 

        \[
            \tau_n - \log(\log(n))  \xrightarrow[n\to\infty]{\mathcal{L}} \mathcal G
        \]
        
    où $\mathcal{G}$ est la loi de Gumbel, de densité $x\mapsto e^{-x - e^{-x}}$.

    \end{theorem}
    La preuve est omise car elle dépasse le cadre de ce rapport. Toutefois, 
    ce théorème renforce l'intuition qu'on a pu commencé à avoir
    à la \autoref{prop:surprise-TMRCA} puisqu'on a que,
    \[
    \lim_{n\to \infty}\mathbb{E}(\tau_n)
    = \lim_{n\to \infty} \log(\log(n)) + \mathbb{E}(\mathcal G)
    = \lim_{n\to \infty} \log(\log(n)) + \gamma
    = +\infty 
    \]
    où $\gamma$ est la constante d'Euler-Mascheroni. En effet,
    comme l'illustre la \autoref{fig:silence-ca-pousse},
    dès $n \approx 50$ on observe $\mathbb{E}(\tau_n) > 2$,
    surpassant la borne du modèle de Kingman $\eqref{eq:tmrca-kingman}$.
    Ainsi, la croissance de la hauteur des arbres généalogiques
    pour le modèle de Bolthausen-Sznitman est extrêmement lente 
    mais permet d'obtenir des arbres aussi grand que l'on souhaite 
    en moyenne.
    % Par exemple en se plaçant de la cadre asymptotique sur 
    % le nombre de lignées initiales $n$, 
    % si on souhaite avoir un arbre
    % de Bolthausen-Sznitman plus grand que celui de Kingman, dont sa hauteur
    % est majorée en moyenne par $2$, il faudrait résoudre 
    % $\log(\log(n))+\gamma \geq 2 \iff n \geq e^{e^{2-\gamma}} \approx 80 $

    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_silence_ca_pousse}
        \caption{(a) \textit{Gauche}~:
        sous le modèle de Bolthausen-Sznitman ($\Lambda$ uniforme)
        nous déterminons les moyennes empiriques de
        $\mathbb{E}(\tau_n)$ (500 répétitions par $n \in 
        \{10, 20, 50, 100, 200, 500, 1e3,2e3,5e3,1e4\}$) comparées à
        l'approximation théorique $\gamma+\log\log n$ et aussi à
        Kingman \eqref{eq:tmrca-kingman}. (b) \textit{Droite}~: histogramme de
        $\tau_n-\log\log n$ pour $n=1000$ (5000 simulations) avec
        superposition de la densité de Gumbel $x\mapsto e^{-x-e^{-x}}$.}
        \label{fig:silence-ca-pousse}
    \end{figure}


 
\bibliographystyle{alpha}
\bibliography{ref}
\end{document}