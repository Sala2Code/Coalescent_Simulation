\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{natbib}
\usepackage{hyperref} % garder en dernier
% Autoref names (français) — évite les warnings de hyperref lorsque
% on utilise \autoref sur des environnements non-standards
\providecommand{\theoremautorefname}{Théorème}
\providecommand{\lemmaautorefname}{Lemme}
\providecommand{\propositionautorefname}{Proposition}
\providecommand{\definitionautorefname}{Définition}
\providecommand{\sectionautorefname}{Section}
\providecommand{\subsectionautorefname}{Sous-section}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{bbm}

\geometry{
    left=1.cm,
    right=1.cm,
    top=1.5cm,
    bottom=1.5cm
}

\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}

\newcommand{\Exp}{\mathrm{Exp}}

\title{Titre du rapport}
\author{Nom de l'auteur}
\date{\today}




\begin{document}

\maketitle

    \section{Introduction}
    \subsection{Fondements du $\Lambda$-coalescent}

    La théorie de la coalescence modélise le phénomène par lequel des
    individus d'une population partagent un ancêtre commun. Nous souhaitons 
    étudier rétrospectivement leur évolution.

    Historiquement,
    le modèle de Wright-Fisher étudie une population de taille finie $N$
    où les individus d'une générations coalescent de manière uniforme entre 
    eux dans la génération précédente \cite{Fisher1930}.
    Ensuite, le modèle de Kingman \cite{Kingman1982} est le modèle
    limite de Wright-Fisher où l'on s'intéresse à $n<N$ lignées
    et en considérant $N\to +\infty$.
    Ce cadre asymptotique permet de
    simplifier grandement l'étude du phénomène de coalescence. 
    Le modèle peut à présent être décrit comme un processus de Markov.

    En 1999, Pitman et Sagitov
    généralisent le modèle de Kingman en autorisant la coalescence
    simultanée de plusieurs lignées. Des individus peuvent engendrer 
    une proportion non négligeable de la population. 
    Afin de définir un modèle, nous supposons raisonnablement
    que les lignées coalescent aléatoirement et indépendamment 
    de leur histoire passée,
    c'est-à-dire en supposant l'absence de mémoire (propriété de Markov),
    que toutes les lignées ont les mêmes chances de coalescer entre elles
    que l'on appelle l'échangeabilité et enfin que nous ayons 
    l'absence de collisions multiples signifiant qu'à tout instant donné,
    il ne peut y avoir qu'un seul événement de fusion en un même ancêtre.


    \begin{theorem}[Pitman-Sagitov {\cite{Pitman1999,Sagitov1999}}]
    Il existe un processus de Markov, appelé $\Lambda$-coalescent,
    échangeable à collisions multiples simples si et seulement s'il
    existe une mesure finie $\Lambda$ sur $[0,1]$ telle que, lorsqu'on a $b$ lignées,
    pour tout $2\le k\le b$ le taux auquel chaque $k$-uplet fixé de lignées
    fusionne vaut, 
    \[
    \lambda_{b,k}=\int_0^1 x^{k-2}(1-x)^{b-k}\,\Lambda(dx)
    \]
    \end{theorem}

    Nous ne définissons par formellement les conditions ici et donnons
    encore moins une preuve car cela est au-delà du cadre de ce rapport.
    Ce résultat montre que la dynamique est entièrement caractérisée
    par une mesure finie, sans perte de généralité nous considérons pour la suite
    une mesure de probabilité, $\Lambda$ sur $[0,1]$. 
    Partant de $b$ lignées,
    le taux d'une $k$-coalescence ($2\leq k \leq b$) est 
    $r_{b,k} \coloneq \binom{b}{k} \lambda_{b,k}$. 
    Le taux de sortie de l'état $b$ est la somme des taux donc
    
    \begin{equation} \label{eq:lambda_b}
        \lambda_b = \sum_{k=2}^{b} r_{b,k} = \int_0^1 S_b(x) \,\Lambda(dx),
        \quad 
        % \text{avec} \quad 
        S_b(x) \coloneq \sum_{k=2}^{b} \binom{b}{k} x^{k-2}(1-x)^{b-k}
        = \frac{1-(1-x)^b - b x (1-x)^{b-1}}{x^2}
    \end{equation}
    
    D'après le lemme des réveils, à chaque événement de coalescence on passe de $b$ à $b-k+1$ lignées
    avec probabilité, 
    \[
    \forall b\geq k \geq 2,
    \quad p_{b,k} \coloneq \frac{r_{b,k}}{\sum_{k=2}^b r_{b,k}} = \frac{\binom{b}{k} \lambda_{b,k}}{\lambda_b}
    \]
    Ainsi, le squelette du processus est une chaîne de Markov
    décroissante sur $\llbracket 1, n \rrbracket$, commençant en $n$ et
    absorbée presque sûrement en $1$.

    \subsection{Exemple (Kingman)} \label{sec:kingman-example}
    Intéressons nous à un cas connu afin de mieux visualiser le modèle.
    \textcolor{red}{
        Là je propose donc (1 page grand max)
        (le but n'est pas de faire une étude de Kingman mais de présenter les différents 
        objets du modèle de manière simple et visuelles)
        \begin{itemize}
            \item  poser $\Lambda = \delta_0$, 
            \item l'intuition du modèle : la masse est vers 0 donc
            pas de grosse fusion. (Inteprétation du modele, du role des termes de l'intégrande)
            \item Les calculs en 5 lignes maximum ($\lambda_{b,k}$, TMRCA si utile, $\dots$ )
            \item Les notations : $n$, $N_t$, $(C_t^i)_{0<i<n}$, TMRCA, $T_k$
        \end{itemize}
    }
    
    \textcolor{red}{
        Subplots (kingman)
        \begin{itemize}
            \item (Une réalisation) Arbre + TMRCA 
            \item (Sur plusieurs réalisaiton) Distribution des fusions ((C\_t\^{}i))\_\{0<i<n \} (donc uniquement en 2 normalement)
            \item  (Sur plusieurs réalisation) distribution TMRCA 
            \item Autres ? 
        \end{itemize}
        Le but est de faire un exemple qui montre quasiement tout ce qu'on va étudier 
        et par la suite quantifier pour beaucoup de mesures.
    }

    \section{Analyse du TMRCA}
    \subsection{Cas extrêmes}

    Au vu du précédent exemple, on peut se demander l'influence de la mesure
    $\Lambda$ sur le TMRCA. Intuitivement, ce temps moyen devrait diminuer lorsque
    la masse de $\Lambda$ se rapproche de 1 puisqu'on autorise des coalescences
    multiples plus importantes. En première analyse on va étudier
    les deux cas extrêmes.

    \begin{lemma}\label{lem:recurrence-TMRCA}
        Notons le TMRCA d'un $\Lambda$-coalescent $\tau\coloneq \inf \{ t\geq 0, N_t=1 \}$.
        Soit $\Lambda$ une mesure de probabilité sur $[0,1]$. Notons
        $H: b \in \mathbb N^* \longmapsto \mathbb{E}_\Lambda(\tau \mid N_0 = b)$.
        Alors $H(1)=0$, $H(2)=1$ et pour $b\geq 3$,
        \[
        H(b) = \frac{1}{\lambda_b} \;+\; \sum_{k=2}^{b-1} p_{b,k}\, H(b-k+1)
        \]
    \end{lemma}

    \begin{proof}
    Pour $b=1$, $N_t=1$  donc $H(1)=0$.
    Pour $b=2$, le seul saut possible est de $2$ vers $1$ lignée
    avec taux $\lambda_2=\binom{2}{2}\lambda_{2,2}=1$, d'où $\tau\sim \Exp(1)$ et $H(2)=1/1=1$.

    Fixons $b\ge 3$. Définissons le temps de la première coalescence,
    \[
    T_1:=\inf\{t\geq0, N_t\neq b\}
    \]

    $(N_t)_{t\geq 0}$ est un processus de Markov avec un taux de saut $\lambda_b$,
    donc $T_1\sim \Exp(\lambda_b)$
    et donc $\mathbb{E}_\Lambda(T_1 \mid N_0 = b)=\frac{1}{\lambda_b}$.
    De plus, si $K$ est la taille de la fusion au temps $T_1$, alors 
    $
    K \sim \sum_{k=2}^{b} p_{b,k} \delta_k
    $
    et $N_{T_1}=b-K+1$.

    Considérons la filtration naturelle $(\mathcal{F}_t)_{t\geq 0}$
    de $(N_t)_{t\geq 0}$. Par la propriété de Markov forte au temps $T_1$ et
    l'absence de mémoire,
    \[
        \mathbb E_\Lambda(\tau-T_1\mid \mathcal F_{T_1}, N_0=b )
        =\mathbb E_\Lambda(\tau \mid N_{T_1})
        =H(N_{T_1})
    \]
    
    Ainsi en conditionnant par $\mathcal{F}_{T_1}$,
    \begin{align*}
        H(b)
        &=\mathbb E_\Lambda(\tau \mid N_0=b)
        =\mathbb E_\Lambda(T_1 \mid N_0=b)+\mathbb E_\Lambda(\tau-T_1 \mid N_0=b)
        % &=\mathbb E_b[T_1]+\mathbb E_b\!\big[\mathbb E_b[\tau-T_1\mid \mathcal F_{T_1}]\big]\\
        =\frac{1}{\lambda_b}+\mathbb E_\Lambda(H(N_{T_1}) \mid N_0=b)\\
        &=\frac{1}{\lambda_b}+\sum_{k=2}^{b}\mathbb P_b\big(N_{T_1}=b-k+1\big) H(b-k+1)
        =\frac{1}{\lambda_b}+\sum_{k=2}^{b} p_{b,k} H(b-k+1)
    \end{align*}

    Or $H(1)=0$, donc le terme $k=b$ s'annule. D'où le résultat.
    \end{proof}

    % \begin{lemma}\label{lem:bounds-Sb}
    %     Pour tout $b\geq 2$ et $x\in[0,1]$,
    %     \[
    %     1 \leq S_b(x) \leq \binom{b}{2}
    %     \]
    % \end{lemma}
    % \begin{proof}
    %     Soit $(X_i)_{1\geq i \geq k}$
    %     de loi de Bernouilli de paramètre $x \in [0,1]$ indépendantes.

    %     Posons $X\coloneq\sum_{i=1}^b X_i \sim \mathrm{Bin}(b,x)$. On a,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \mathbb{E}(\sum_{i\neq j} X_i X_j)
    %     \overset{\perp\!\!\!\perp}{=}\sum_{i\neq j} \mathbb{E}(X_i)\mathbb{E}(X_j)  
    %     = b(b-1)x^2
    %     \]

    %     En utilisant que $k(k-1) \geq 2 \mathbbm{1}_{k\geq 2}$,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \sum_{k=0}^b k(k-1) \binom{b}{k} x^k (1-x)^{b-k}
    %     \geq \sum_{k=2}^b 2 \binom{b}{k} x^k (1-x)^{b-k}
    %     = 2 S_b(x)x^2
    %     \]

    %     D'où,
    %     \[
    %     2S_b(x)x^2 \leq b(b-1)x^2 
    %     \Longleftrightarrow
    %     S_b(x) \leq \frac{b(b-1)}{2} = \binom{b}{2}
    %     \]

    %     Pour l'autre inégalité, on remarque que pour tout $x\in]0,1]$,
    %     \[
    %     S_b(x) = \frac{1}{x^2}\mathbb P(X\geq 2)
    %     \geq \frac{1}{x^2} \mathbb{P}(X_1=X_2=1) = \frac{x^2}{x^2} = 1
    %     \]

    %     Puis $S_b(0) = \binom{b}{2} \geq 1$, d'où le résultat.
    % \end{proof}

    \begin{proposition}
        Soit $n$ le nombre de lignées. Notons le TMRCA d'un $\Lambda$-coalescent,
        \[
        \tau\coloneq \inf \{ t\geq 0, N_t=1 \}
        \]

        Alors, pour toute mesure de probabilité $\Lambda$ sur $[0,1]$, on a
        condtionnellement à $\{N_0 = n\}$,
        \[
        1 = \mathbb{E}_{\delta_1}(\tau) 
        \leq \mathbb{E}_{\Lambda}(\tau)
        % \leq\mathbb{E}_{\delta_0}(\tau) = 2\left(1-\frac{1}{n}\right)
        \]
    \end{proposition}
    \begin{proof}
    % L'égalité de droite a été montrée dans l'exemple de Kingman (voir \autoref{sec:kingman-example}).
    Prouvons l'égalité. Prenons $\Lambda = \delta_1$, nous avons $\lambda_{n,k} = \delta_{n,k}$ (symbole de Kronecker),
    donc $\lambda_n = \binom{n}{n} \lambda_{n,n} = 1$ donc
    $\tau \sim \Exp(1)$ et donc $\mathbb{E}_{\delta_1}(\tau) = 1/1 = 1$.
    
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$.
    Notons $H(b) \coloneq \mathbb{E}_\Lambda(\tau \mid N_0 = b)$.
    % D'après \eqref{eq:lambda_b}, 
    % D'après la propriété de Markov et l'absence de mémoire de l'exponentielle,
    % \[
    % H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    % \]
    
    Montrons par récurrence forte l'inégalité, c'est-à-dire $H(b)\geq 1$
    pour $b\geq 2$.
    % Pour $b=2$, $\lambda_{2,2}=1$ donc $\tau \sim \Exp(1)$
    % donc $H(2) = 1 \geq 1$. 
    L'initialisation a été prouvée dans le lemme \ref{lem:recurrence-TMRCA}.
    Supposons l'inégalité vraie jusqu'à $b-1$. 
    Remarquons que $\lambda_{b,b} = \int_0^1 x^{b-2}\,\Lambda(dx) \leq \int_0^1 \Lambda(dx) = 1$,
    \[
    H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    \geq \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} 
    = \frac{1}{\lambda_b} + 1 - p_{b,b}
    = 1 + \frac{1 - \lambda_{b,b}}{\lambda_b} \geq 1
    \]

    D'où le résultat.
    \end{proof}

    Cette idée de déplacer la masse de $\Lambda$ vers 1 pour diminuer la moyenne 
    du TMRCA
    est intuitive. Pour le problème inverse de maximisation du TMRCA nous
    souhaiterions déplacer la masse de $\Lambda$ vers 0. C'est-à-dire prouver que 
    le modèle de Kingman soit celui maximisant le temps moyen du TMRCA. 
    Toutefois, voila une grande surprise : ce n'est pas le cas !

    \begin{proposition}\label{prop:surprise-TMRCA}
        Il existe $n>1$ et une mesure de probabilité $\Lambda$ sur $[0,1]$
        telle que, conditionnellement à $\{N_0=n\}$,
        \[
        \mathbb{E}_{\Lambda}(\tau)> \mathbb{E}_{\delta_0}(\tau)
        \]
    \end{proposition}
    \begin{proof}
        Soit $n=8$, dans l'exemple de Kingman (voir \autoref{sec:kingman-example}),
        nous avons une formule explicite.
        \[\
        \mathbb E_{\delta_0}(\tau) = 2\left(1-\frac{1}{8}\right) = \frac{14}{8} = 1.75
        \]

        Soit $\Lambda = \delta_{1/4}$, alors d'après \eqref{eq:lambda_b},
        \[ 
        \lambda_{n,k} = \left(\frac{1}{4}\right)^{k-2} \left(\frac{3}{4}\right)^{n-k}
        \quad 
        \lambda_n = 16\left(1 -\left(\frac34\right)^n - \frac{n}{4}\left(\frac34\right)^{n-1}\right)
        \] 
        
        Ainsi, en calculant nous obtenons,
        \[ 
        \mathbb{E}_{\delta_{1/4}}(\tau)
        = \frac{1}{\lambda_n} + \sum_{k=2}^{n-1} \frac{\binom{n}{k} \lambda_{n,k}}{\lambda_n} \mathbb{E}_{\delta_{1/4}}(\tau \mid N_0 = n-k+1)
        = \frac{19954284839411683}{11337879079537330} > 1.7599662 \dotsc > 1.75
        \]
    \end{proof}
    Nous conjecturons que le théorème peut être étendu pour tout $n>6$.
    A notre connaissance l'étude ce phénomène n'est pas documenté pour $n$ fini.
    Seul un article de \cite{kluge2017exchangeable} s'intéresse
    la croissance de $ \sup_\Lambda \mathbb{E}_\Lambda(\tau)$ lorsque $n\to\infty$.


    \textcolor{red}{Là on a une belle expérience numérique à faire !!
    On peut faire un plot de la distribution pour ces 2 mesures et voir qu'en moyenne
    on a bien delta1/4 devant delta0 ! + intervalle asymtptotique si possible}
    



   \textcolor{blue}{L'échelle de temps ici est en unités de $N$ générations, avec $N\gg n$ puisque
    nous considérons un modèle asymptotique.}

    % TODO : Reformuler !
    % Une suprise ... À notre connaissance,
    % la question de déterminer, pour un (n) fixé,
    % la mesure (\Lambda) qui maximise l’espérance du temps
    % d’absorption (\mathbb E_\Lambda[\tau]) du (\Lambda)-coalescent
    % reste ouverte. Les travaux existants se concentrent sur les lois
    % limites et les constantes asymptotiques de (\tau_n) lorsque
    % (n\to\infty) (voir p.ex. Kersting–Wakolbinger), sans adresser
    % l’extrémalité en (\Lambda) pour (n) fini.


    \subsection{Une forêt pas si grande }

    Un processus de Markov est entièrement déterminé
    par son générateur infinitésimal.
    Pour $n$ lignées observées,
    celui d'un $\Lambda$-coalescent
    est la matrice triangulaire inférieure
    $Q \in \mathcal M_n(\mathbb{R})$ définie 
    pour tout $1\leq b,i\leq n$, par
    \[
        Q_{b,i} =
        \begin{cases}
        r_{b,k} & \text{si } b\ge 2 \text{ et } i = b-k+1 \text{ pour } 2\le k\le b\\
        -\lambda_b & \text{si } b\ge 2 \text{ et } i = b\\
        0 & \text{sinon}
        \end{cases}
    \]

    Le premier élément de sa diagonale, $Q_{1,1}$, est nul
    car l'état $1$ est absorbant donc $Q$ n'est pas inversible.
    On remarque également que ce processus est défini 
    par $(\lambda_{b,k})_{I_n}$ avec
    $I_n \coloneq \{(b,k), 2\leq k \leq b \leq n \}$.
    Définissons pour $r \in \llbracket 0, n-2 \rrbracket$
    \[ 
    m_r : \Lambda \mapsto \int_0^1 x^r \Lambda(dx)
    \]

    En développant l'intégrande des taux de fusions, 
    pour tout $(b,k) \in I_n$,
    il existe $A_n \in \mathcal M_{I_n, n-1}(\mathbb{R})$ tel que, 
    \[
    \lambda_{b,k} = \sum_{r=0}^{n-2} A_{(b,k),r} m_r(\Lambda)
    \]
        
    Pour $n$ fixé, $Q$ est entièrement déterminée
    par $(m_r(\Lambda))_{0\leq r \leq n-2}$,
    l'espace
    des mesures de probabilité sur $[0,1]$ se réduit
    à une projection
    de dimension finie, $\mathbb{R}^{n-1}$, donc un espace bien plus petit.
    Autrement dit, 
    une infinité de mesures différentes
    deviennent indiscernables pour un processus considéré.
    
    \begin{proposition}\label{}
        Soit $n>1$. Notons $P_k$ le polynôme de 
        Legendre de degré $k$. 
        Prenons $\Lambda_1$ la mesure uniforme 
        sur $[0,1]$. Pour tout $0<\varepsilon<1$,
        définissons la mesure de probabilité
        $\Lambda_\varepsilon \neq \Lambda_1$
        de densité sur $[0,1]$
        \[ 
        f_\varepsilon(x) = 1 + \varepsilon P_{n-1}(2x-1)
        \]

        Alors, pour tout $0<\varepsilon<1$, 
        \[
        \tau_{\Lambda_1} \overset{\mathcal{L}}{=} \tau_{\Lambda_\varepsilon}
        \]

    \end{proposition}
    \begin{proof}
        Montrons que pour tout $0<\varepsilon<1$,
        $f_\varepsilon$ est bien une densité de probabilité.
        Nous avons, par une analyse élémentaire que
        pour tout $k\geq 0$
        $|P_k|\leq 1$ sur $[-1,1]$
        \cite{WhittakerWatson1920} donc $f_\epsilon \geq 1-1\cdot\varepsilon \geq0 $.
        De plus, $(P_k)_{0\leq k \leq n}$ est une base orthogonale de 
        $\mathbb{R}_n[X]$ pour le produit scalaire 
        $f,g \mapsto \int_{-1}^1 f(x)g(x) dx$
        donc $\int_0^1 f_\varepsilon(x)dx =
        1+\frac{\varepsilon}{2} \int_{-1}^1 1\cdot P_n(x) dx = 1+0 = 1$. 

        Montrons à présent que le générateur infinitésimal
        de $\Lambda_1$ et $\Lambda_\varepsilon$ sont identiques.
        Soit $r \in \llbracket 0, n-2 \rrbracket$,
        puisque $X^r \in \mathbb{R}_{n-2}[X]
        \subset \mathbb{R}_{n-1}[X]$,
        \[
        m_r(\Lambda_\varepsilon)
        = \int_0^1 x^r f_\varepsilon(x) dx
        = \int_0^1 x^r dx + \varepsilon \int_0^1 x^r P_{n-1}(2x-1) dx
        = \frac{1}{r+1} + \varepsilon \cdot 0
        = m_r(\Lambda_1)
        \]

        Les taux de fusions sont donc égaux entre ces mesures,
        d'où le résultat.
    \end{proof}

    Ainsi nous venons de construire une infinité de mesures 
    différentes qui induisent le même processus de coalescence.
    On s'attendait à obtenir une infinité d'arbres généalogiques
    différents mais ceux-ci sont identiques en loi.

    \textcolor{red}{subplots : prendre n=10, epsilon = 0.5 (ou autre)
    \begin{itemize}
        \item  densité de $\Lambda_\epsilon$ pour plusieurs $\epsilon$
        \item  distribution du TMRCA pour ces différentes mesures et loi uniforme
        (à voir comment bien l'afficher)
        \item erreur de leur distribution qui devrait être de l'ordre de 
        l'erreur numérique.
    \end{itemize}
    }

    \subsection{Silence, ça pousse}
    Précedemment nous avons parlé de la mesure uniforme.
    Ce modèle est connu sous le nom de Bolthausen-Sznitman
    et décèle un résultat incontournable.
    % \begin{theorem}[C. Goldschmidt \& J. B. Martin \cite{goldschmidt-martin-2005}]
    \begin{theorem}[Goldschmidt \& Martin~\cite{goldschmidt-martin-2005}]
        Soit $(N_t)_{t\geq 0}$ un Bolthausen-Sznitman coalescent
        Notons, pour $n>0$,
        $ \tau_n \coloneq \inf\{t\geq 0, N_0 = n, N_t=1\}$. Alors, 

        \[
            \tau_n - \log(\log(n))  \xrightarrow[n\to\infty]{\mathcal{L}} \mathcal G
        \]
        
    où $\mathcal{G}$ est la loi de Gumbel, de densité $x\mapsto e^{-x - e^{-x}}$.

    \end{theorem}
    La preuve est omise car elle dépasse le cadre de ce rapport. Toutefois, 
    ce théorème renforce l'intuition qu'on a pu commencé à avoir
    à la \autoref{prop:surprise-TMRCA} puisqu'on a que,
    \[
    \lim_{n\to \infty}\mathbb{E}(\tau_n)
    = \lim_{n\to \infty} \log(\log(n)) + \mathbb{E}(\mathcal G)
    = \lim_{n\to \infty} \log(\log(n)) + \gamma
    = +\infty 
    \]
    où $\gamma$ est la constante d'Euler-Mascheroni.
    Ainsi, la croissance de la hauteur des arbres généalogiques
    pour le modèle de Bolthausen-Sznitman est extrêmement lente 
    mais permet d'obtenir des arbres aussi grand que l'on souhaite 
    en moyenne.
    % Par exemple en se plaçant de la cadre asymptotique sur 
    % le nombre de lignées initiales $n$, 
    % si on souhaite avoir un arbre
    % de Bolthausen-Sznitman plus grand que celui de Kingman, dont sa hauteur
    % est majorée en moyenne par $2$, il faudrait résoudre 
    % $\log(\log(n))+\gamma \geq 2 \iff n \geq e^{e^{2-\gamma}} \approx 80 $

    \textcolor{red}{Un plot où on affiche $\tau$ qu'on soustrait à loglog(n) 
    et on comparer avec la densité de Gumbel.
    On peut aussi faire un autre subplot afin de voir la croissance asymtpoquie en log log n.}


\bibliographystyle{alpha}
\bibliography{ref}
\end{document}