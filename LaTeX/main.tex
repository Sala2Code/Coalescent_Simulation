\documentclass[12pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{caption}
\usepackage{natbib}
\usepackage{hyperref} % garder en dernier
\usepackage{svg}
\usepackage{float}

\svgsetup{
  inkscapelatex=true,
  inkscapeexe=inkscape,
  clean=true
}
% Set a default search path for svg figures (so \includesvg{plot_silence_ca_pousse} works)
\svgpath{{photo/}}
% Autoref names (français) — évite les warnings de hyperref lorsque
% on utilise \autoref sur des environnements non-standards
\providecommand{\theoremautorefname}{Théorème}
\providecommand{\lemmaautorefname}{Lemme}
\providecommand{\propositionautorefname}{Proposition}
\providecommand{\definitionautorefname}{Définition}
\providecommand{\sectionautorefname}{Section}
\providecommand{\subsectionautorefname}{Sous-section}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{bbm}

\geometry{
    left=1.cm,
    right=1.cm,
    top=1.5cm,
    bottom=1.5cm
}

\newtheorem{theorem}{Théorème}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}

\newcommand{\Exp}{\mathrm{Exp}}

\title{Analyse du $\Lambda$-coalescent : renouer avec ses racines. }
\author{SALA Raphaël, MUGISHA Axcel, GARCIA Hugo, COLIN Thibault}
\makeatletter
\newcommand{\monthandyear}{%
    \ifcase\month\or janvier\or février\or mars\or avril\or mai\or juin\or juillet\or août\or septembre\or octobre\or Novembre\or Décembre\fi\ \number\year}
\makeatother
\date{\monthandyear}




\begin{document}

\maketitle

    \section{Introduction}
    \subsection{Fondements du $\Lambda$-coalescent}

    La théorie de la coalescence modélise le phénomène par lequel des
    individus d'une population partagent un ancêtre commun. Nous souhaitons 
    étudier rétrospectivement leur évolution.

    Historiquement,
    le modèle de Wright-Fisher étudie une population de taille finie $N$
    où les individus d'une générations coalescent de manière uniforme entre 
    eux dans la génération précédente \cite{Fisher1930}.
    Ensuite, le modèle de Kingman \cite{Kingman1982} est le modèle
    limite de Wright-Fisher où l'on s'intéresse à $n<N$ lignées
    et en considérant $N\to +\infty$.
    Ce cadre asymptotique permet de
    simplifier grandement l'étude du phénomène de coalescence. 
    Le modèle peut à présent être décrit comme un processus de Markov.

    En 1999, Pitman et Sagitov
    généralisent le modèle de Kingman en autorisant la coalescence
    simultanée de plusieurs lignées. Des individus peuvent engendrer 
    une proportion non négligeable de la population. 
    Afin de définir un modèle, nous supposons raisonnablement
    que les lignées coalescent aléatoirement et indépendamment 
    de leur histoire passée,
    c'est-à-dire en supposant l'absence de mémoire (propriété de Markov),
    que toutes les lignées ont les mêmes chances de coalescer entre elles
    que l'on appelle l'échangeabilité et enfin que nous ayons 
    l'absence de collisions multiples signifiant qu'à tout instant donné,
    il ne peut y avoir qu'un seul événement de fusion en un même ancêtre.


    \begin{theorem}[Pitman-Sagitov {\cite{Pitman1999,Sagitov1999}}]
    Il existe un processus de Markov, $(N_t)_{t\geq 0}$, appelé $\Lambda$-coalescent,
    échangeable à collisions multiples simples si et seulement s'il
    existe une mesure finie $\Lambda$ sur $[0,1]$ telle que, lorsqu'on a $b$ lignées,
    pour tout $2\le k\le b$ le taux auquel chaque $k$-uplet fixé de lignées
    fusionne vaut, 
    \[
    \lambda_{b,k}=\int_0^1 x^{k-2}(1-x)^{b-k}\,\Lambda(dx)
    \]
    \end{theorem}

    Nous ne définissons par formellement les conditions ici et donnons
    encore moins une preuve car cela est au-delà du cadre de ce rapport.
    Ce résultat montre que la dynamique de $(N_t)_{t\geq 0}$,
    indiquant le nombre de lignées à l'instant $t$, est entièrement caractérisée
    par une mesure finie. Sans perte de généralité nous considérons pour la suite
    une mesure de probabilité, $\Lambda$ sur $[0,1]$. 
    Partant de $b$ lignées,
    le taux d'une $k$-coalescence ($2\leq k \leq b$) est 
    $r_{b,k} \coloneq \binom{b}{k} \lambda_{b,k}$. 
    Le taux de sortie de l'état $b$ est la somme des taux donc
    
    \begin{equation} \label{eq:lambda_b}
        \lambda_b = \sum_{k=2}^{b} r_{b,k} = \int_0^1 S_b(x) \,\Lambda(dx),
        \quad
        S_b(x) \coloneq \sum_{k=2}^{b} \binom{b}{k} x^{k-2}(1-x)^{b-k}
        = \frac{1-(1-x)^b - b x (1-x)^{b-1}}{x^2}
    \end{equation}
    
    D'après le lemme des réveils, à chaque événement de coalescence on passe de $b$ à $b-k+1$ lignées
    avec probabilité, 
    \[
    \forall b\geq k \geq 2,
    \quad p_{b,k} \coloneq \frac{r_{b,k}}{\sum_{k=2}^b r_{b,k}} = \frac{\binom{b}{k} \lambda_{b,k}}{\lambda_b}
    \]
    Ainsi, le squelette du processus est une chaîne de Markov
    décroissante sur $\llbracket 1, n \rrbracket$, commençant en $n$ et
    absorbée presque sûrement en $1$.

    \subsection{Exemple (Kingman)} \label{sec:kingman-example}
        Intéressons-nous au modèle de Kingman en guise d'introduction. 
    On pose $\Lambda = \delta_0$.
    Pour $2\leq k\leq b $,
    \[
    \lambda_{b,k}
        = \int_0^1 x^{k-2}(1-x)^{b-k} \delta_0(x) dx
        = [x^{k-2}(1-x)^{b-k}]_{x=0} 
        =
    \begin{cases}
    (1-0)^{b-2} = 1 & \text{si } k = 2 \\
    0^{k-2}(1-0)^{b-k} = 0 & \text{si } k > 2
    \end{cases}
    \]

    Les coalescences se font que par paires. Une caractéristique intéressante
    du $\Lambda$-coalescent
    est le TMRCA (Time to the Most Recent Common Ancestor),
    c'est-à-dire le plus petit temps tel que toutes les lignées
    ont fusionné en un ancêtre commun. Dans la suite du rapport, nous le notons
    \[
    \tau_{\Lambda,n} \coloneq \inf \{ t\geq 0, N_t=1 \} \mid \{N_0 = n\}
    \]
    Lorsque le contexte est clair sur $\Lambda$ ou $n$, ceux-ci seront omis 
    afin de rendre la lecture plus agréable.
    
    \begin{lemma}\label{lem:recurrence-TMRCA}
        % Notons le TMRCA d'un $\Lambda$-coalescent $\tau\coloneq \inf \{ t\geq 0, N_t=1 \}$.
        Soit $\Lambda$ une mesure de probabilité sur $[0,1]$. Notons
        $H: b \in \mathbb N^* \longmapsto \mathbb{E}(\tau_{\Lambda, b})$.
        Alors $H(1)=0$, $H(2)=1$ et pour $b\geq 3$,
        \[
        H(b) = \frac{1}{\lambda_b} \;+\; \sum_{k=2}^{b-1} p_{b,k}\, H(b-k+1)
        \]
    \end{lemma}

    \begin{proof}
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$
    dont nous omettons sa présence dans les notations.
    Pour $b=1$, $N_t=1$  donc $H(1)=0$.
    Pour $b=2$, le seul saut possible est de $2$ vers $1$ lignée
    avec taux $\lambda_2=\binom{2}{2}\lambda_{2,2}=1$, d'où $\tau_{2} \sim \Exp(1)$ et $H(2)=1/1=1$.

    Fixons $b\ge 3$. Définissons le temps de la première coalescence,
    \[
    T_b^1:=\inf\{t\geq0, N_t\neq b\} \mid \{N_0 = b\}
    \]

    $(N_t)_{t\geq 0}$ est un processus de Markov avec un taux de saut $\lambda_b$,
    donc $T_b^1\sim \Exp(\lambda_b)$
    et donc $\mathbb{E}(T_b^1)=\frac{1}{\lambda_b}$.
    De plus, si $K$ est la taille de la fusion au temps $T_b^1$, alors 
    $
    K \sim \sum_{k=2}^{b} p_{b,k} \delta_k
    $
    et $N_{T_b^1}=b-K+1$.

    Considérons la filtration naturelle $(\mathcal{F}_t)_{t\geq 0}$
    de $(N_t)_{t\geq 0}$. Par la propriété de Markov forte au temps $T_b^1$ et
    l'absence de mémoire,
    \[
        \mathbb E(\tau_b-T_b^1\mid \mathcal F_{T_b^1})
        =\mathbb E(\tau_{N_{T_b^1}})
        =H(N_{T_b^1})
    \]
    
    Ainsi en conditionnant par $\mathcal{F}_{T_1}$,
    \begin{align*}
        H(b)
        &=\mathbb E(\tau_b)
        =\mathbb E(T_b^1)+\mathbb E(\tau_b-T_b^1)
        % &=\mathbb E_b[T_1]+\mathbb E_b\!\big[\mathbb E_b[\tau-T_1\mid \mathcal F_{T_1}]\big]\\
        =\frac{1}{\lambda_b}+\mathbb E \bigl( \mathbb E(\tau_b-T_b^1\mid \mathcal F_{T_b^1})  \bigr)
        =\frac{1}{\lambda_b}+\mathbb E(H(N_{T_b^1})) \\
        &=\frac{1}{\lambda_b}+\sum_{k=2}^{b}\mathbb P\big(N_{T_b^1}=b-k+1\big) H(b-k+1)
        =\frac{1}{\lambda_b}+\sum_{k=2}^{b} p_{b,k} H(b-k+1)
    \end{align*}

    Or $H(1)=0$, donc le terme $k=b$ s'annule. D'où le résultat.
    \end{proof}

    
    
    
    Pour $b$ lignées observées, on a
    $ \lambda_b = \sum_{k=2}^{b} \binom{b}{k} \lambda_{b,k}
               = \binom{b}{2} \lambda_{b,2}
               = \binom{b}{2}
        $
    donc, d'après le \autoref{lem:recurrence-TMRCA}, la taille moyenne d'un arbre pour
    le modèle de Kingman est donné par,
    \[
    H(b) = \frac{1}{\lambda_b} + p_{b,2}\,H(b-1)
    = \frac{1}{\binom{b}{2}} + H(b-1)
    \]

    Ainsi par récurrence,
    \begin{equation}\label{eq:tmrca-kingman}
        H(b)=\sum_{k=2}^{b}\frac{1}{\binom{k}{2}}
        = \sum_{k=2}^{b}\frac{2}{k(k-1)}
        = \sum_{k=2}^{b}2\Big(\frac{1}{k-1}-\frac{1}{k}\Big)
        =2\Big(1-\frac{1}{b}\Big)
    \end{equation}
    
    
    Nous illustrons ce résultat par une simulation pour $n=20$ lignées (Fig. \ref{fig:kingman_simu}). 
    On observe bien que les fusions sont binaires et que le temps total de coalescence oscille autour de la valeur théorique $2(1 - 1/20) = 1.9$.

\begin{figure}[H]
    \centering
    \includesvg[width=\textwidth]{photo/kingmanplot}
    \caption{Simulation de Kingman ($n=20$).
    \textbf{À droite} : Une trajectoire du Kingman-coalescent.
    \textbf{Au centre} : La généalogie réalisée, montrant les fusions par paires pour la trajectoire (à droite).
    \textbf{À gauche} : La densité du TMRCA. La ligne pointillée grise indique le TMRCA de la trajectoire (à droite)(\ref{eq:tmrca-kingman})\textcolor{red}{et si à la place tu faisait une simulation et générer un histogramme et parler aussi un peu de la comment tu as simuler tous ça }.}
    \label{fig:kingman_simu}
\end{figure}
    
%\textcolor{red}{
%    Subplots (kingman)
%    \begin{itemize}
%        \item (Une réalisation) Arbre + TMRCA 
%        \item (Sur plusieurs réalisaiton) Distribution des fusions ((C\_t\^{}i))\_\{0<i<n \} (donc uniquement en 2 normalement)
%        \item  (Sur plusieurs réalisation) distribution TMRCA + densité avec en légende "densité théorique"
%        dans le caption mettre, ou en footnote, que la densité est explcite dans ce cas dont la formule 
%        n'est pas prouvée, ele peut être écritre matriciellement d'après (citer "une foret pas si grande")
%    \end{itemize}
%}

    \section{Analyse du TMRCA}
    \subsection{Aux extrêmes de l'arbre. }

    Au vu du précédent exemple, on peut se demander l'influence de la mesure
    $\Lambda$ sur le TMRCA. Intuitivement, ce temps moyen devrait diminuer lorsque
    la masse de $\Lambda$ se rapproche de 1 puisqu'on autorise des coalescences
    multiples plus importantes. En première analyse on va étudier
    les deux cas extrêmes.

    % \begin{lemma}\label{lem:bounds-Sb}
    %     Pour tout $b\geq 2$ et $x\in[0,1]$,
    %     \[
    %     1 \leq S_b(x) \leq \binom{b}{2}
    %     \]
    % \end{lemma}
    % \begin{proof}
    %     Soit $(X_i)_{1\geq i \geq k}$
    %     de loi de Bernouilli de paramètre $x \in [0,1]$ indépendantes.

    %     Posons $X\coloneq\sum_{i=1}^b X_i \sim \mathrm{Bin}(b,x)$. On a,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \mathbb{E}(\sum_{i\neq j} X_i X_j)
    %     \overset{\perp\!\!\!\perp}{=}\sum_{i\neq j} \mathbb{E}(X_i)\mathbb{E}(X_j)  
    %     = b(b-1)x^2
    %     \]

    %     En utilisant que $k(k-1) \geq 2 \mathbbm{1}_{k\geq 2}$,
    %     \[
    %     \mathbb{E}(X(X-1))
    %     = \sum_{k=0}^b k(k-1) \binom{b}{k} x^k (1-x)^{b-k}
    %     \geq \sum_{k=2}^b 2 \binom{b}{k} x^k (1-x)^{b-k}
    %     = 2 S_b(x)x^2
    %     \]

    %     D'où,
    %     \[
    %     2S_b(x)x^2 \leq b(b-1)x^2 
    %     \Longleftrightarrow
    %     S_b(x) \leq \frac{b(b-1)}{2} = \binom{b}{2}
    %     \]

    %     Pour l'autre inégalité, on remarque que pour tout $x\in]0,1]$,
    %     \[
    %     S_b(x) = \frac{1}{x^2}\mathbb P(X\geq 2)
    %     \geq \frac{1}{x^2} \mathbb{P}(X_1=X_2=1) = \frac{x^2}{x^2} = 1
    %     \]

    %     Puis $S_b(0) = \binom{b}{2} \geq 1$, d'où le résultat.
    % \end{proof}

    \begin{proposition}
        Soit $n$ le nombre de lignées. 
        % Notons le TMRCA d'un $\Lambda$-coalescent,
        % \[
        % \tau\coloneq \inf \{ t\geq 0, N_t=1 \}
        % \]
        Alors, pour toute mesure de probabilité $\Lambda$ sur $[0,1]$, on a,
        \[
        1 = \mathbb{E}(\tau_{\delta_1,n}) 
        \leq \mathbb{E}(\tau_{\Lambda,n})
        % \leq\mathbb{E}_{\delta_0}(\tau) = 2\left(1-\frac{1}{n}\right)
        \]
    \end{proposition}
    \begin{proof}
    % L'égalité de droite a été montrée dans l'exemple de Kingman (voir \autoref{sec:kingman-example}).
    Prouvons l'égalité. Prenons $\Lambda = \delta_1$, nous avons $\lambda_{n,k} = \delta_{n,k}$ (symbole de Kronecker),
    donc $\lambda_n = \binom{n}{n} \lambda_{n,n} = 1$ donc
    $\tau_{\delta_1} \sim \Exp(1)$ et donc $\mathbb{E}(\tau_{\delta_1}) = 1/1 = 1$.
    
    Soit $\Lambda$ une mesure de probabilité sur $[0,1]$.
    Notons $H(b) \coloneq \mathbb{E}(\tau_{\Lambda, b})$.
    % D'après \eqref{eq:lambda_b}, 
    % D'après la propriété de Markov et l'absence de mémoire de l'exponentielle,
    % \[
    % H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    % \]
    
    Montrons par récurrence forte l'inégalité, c'est-à-dire $H(b)\geq 1$
    pour $b\geq 2$.
    % Pour $b=2$, $\lambda_{2,2}=1$ donc $\tau \sim \Exp(1)$
    % donc $H(2) = 1 \geq 1$. 
    L'initialisation a été prouvée dans le lemme \ref{lem:recurrence-TMRCA}.
    Supposons l'inégalité vraie jusqu'à $b-1$. 
    Remarquons que $\lambda_{b,b} = \int_0^1 x^{b-2}\,\Lambda(dx) \leq \int_0^1 \Lambda(dx) = 1$,
    \[
    H(b) = \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} H(b-k+1)
    \geq \frac{1}{\lambda_b} + \sum_{k=2}^{b-1} p_{b,k} 
    = \frac{1}{\lambda_b} + 1 - p_{b,b}
    = 1 + \frac{1 - \lambda_{b,b}}{\lambda_b} \geq 1
    \]

    D'où le résultat.
    \end{proof}

    Cette idée de déplacer la masse de $\Lambda$ vers 1 pour diminuer la moyenne 
    du TMRCA
    est intuitive. Pour le problème inverse de maximisation du TMRCA nous
    souhaiterions déplacer la masse de $\Lambda$ vers 0. C'est-à-dire prouver que 
    le modèle de Kingman soit celui maximisant le temps moyen du TMRCA. 
    Toutefois, voila une grande surprise : ce n'est pas le cas !

    \begin{proposition}\label{prop:surprise-TMRCA}
        Il existe $n>1$ et une mesure de probabilité $\Lambda$ sur $[0,1]$
        telle que, 
        \[
        \mathbb{E}(\tau_{\Lambda,n})> \mathbb{E}(\tau_{\delta_0,n})
        \]
    \end{proposition}
    \begin{proof}
        Soit $n=8$, dans l'exemple de Kingman (voir \autoref{sec:kingman-example}),
        nous avons une formule explicite.
        \[\
        \mathbb E(\tau_{\delta_0}) = 2\left(1-\frac{1}{8}\right) = \frac{14}{8} = 1.75
        \]

        Soit $\Lambda = \delta_{1/4}$, alors d'après \eqref{eq:lambda_b},
        \[ 
        \lambda_{n,k} = \left(\frac{1}{4}\right)^{k-2} \left(\frac{3}{4}\right)^{n-k}
        \quad 
        \lambda_n = 16\left(1 -\left(\frac34\right)^n - \frac{n}{4}\left(\frac34\right)^{n-1}\right)
        \] 
        
        Ainsi, en calculant nous obtenons, toujours pour $n=8$,
        \[ 
        \mathbb{E}(\tau_{\delta_{1/4},n})
        = \frac{1}{\lambda_n} + \sum_{k=2}^{n-1} \frac{\binom{n}{k} \lambda_{n,k}}{\lambda_n} \mathbb{E}(\tau_{\delta_{1/4},n-k+1} )
        = \frac{19954284839411683}{11337879079537330} \approx 1.7599662 \dotsc > 1.75
        \]
    \end{proof}
    Nous conjecturons que le théorème peut être étendu pour tout $n>6$.
    A notre connaissance l'étude ce phénomène n'est pas documenté pour $n$ fini.
    Seul un article de \cite{kluge2017exchangeable} s'intéresse
    la croissance de $ \sup_\Lambda \mathbb{E}(\tau_\Lambda)$ lorsque $n\to\infty$.


      \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_tmrca_extrema_histograms}
        \caption{Distribution empirique du TMRCA pour $n=8$
        de $\Lambda=\delta_0$ (Kingman, histogramme vers le haut en bleu)
        et $\Lambda=\delta_{1/4}$ (histogramme vers le bas en orange).
        Les bandes verticales pointillées délimitent les intervalles
        de confiance (IC) à $95\%$ pour la moyenne de
        $\tau$ basés sur $M=1e5$ simulations
        indépendantes. Les lignes pleines indiquent les moyennes
        empiriques et les lignes pointillées ("-.")
        les moyennes théoriques.}
        \label{fig:tmrca-extrema}
    \end{figure}

    Dans la \autoref{fig:tmrca-extrema} nous utilisons des intervalles
    de confiances.
    Soit $(T_i)_{1\leq i \leq M}$ une collection de variables 
    aléatoires i.i.d. de loi $\tau_\Lambda$. Posons la moyenne et
    l'estimateur de la variance
    \[
        \overline T_M \coloneq \frac{1}{M}\sum_{i=1}^M T_i\qquad
        s_M^2 \coloneq \frac{1}{M-1}\sum_{i=1}^M (T_i - \overline T_M)^2
    \]
    
    Les intervalles de confiances sont construits génériquement.
    D'après le théorème central limite et le lemme de Slutsky,
    \[
        \sqrt{M}\frac{\overline T_M -
        \mathbb{E}_\Lambda[\tau]}{\sqrt{s^2_M}}
        \xrightarrow[M\to\infty]{\mathcal{L}} \mathcal N(0,1)
    \]

    Ainsi, un IC asymptotique de niveau $1-\alpha$ est
    \[
        [ \overline T_M \pm q_{1-\alpha/2}
        \frac{\sqrt{s^2_M}}{\sqrt{M}}]
    \]

    Dans la \autoref{fig:tmrca-extrema} 
    le zoom à droite montre les intervalles de confiance à $95\%$ disjoints
    renforçant l'observation $\mathbb E(\tau_{\delta_0, 8}) \leq
    \mathbb E(\tau_{\delta_{1/4}, 8})  $.


   \textcolor{blue}{L'échelle de temps ici est en unités de $N$ générations, avec $N\gg n$ puisque
    nous considérons un modèle asymptotique.}

    % TODO : Reformuler !
    % Une suprise ... À notre connaissance,
    % la question de déterminer, pour un (n) fixé,
    % la mesure (\Lambda) qui maximise l’espérance du temps
    % d’absorption (\mathbb E_\Lambda[\tau]) du (\Lambda)-coalescent
    % reste ouverte. Les travaux existants se concentrent sur les lois
    % limites et les constantes asymptotiques de (\tau_n) lorsque
    % (n\to\infty) (voir p.ex. Kersting–Wakolbinger), sans adresser
    % l’extrémalité en (\Lambda) pour (n) fini.


    \subsection{Une forêt pas si grande }

    Un processus de Markov est entièrement déterminé
    par son générateur infinitésimal.
    Pour $n$ lignées observées,
    celui d'un $\Lambda$-coalescent
    est la matrice triangulaire inférieure
    $Q \in \mathcal M_n(\mathbb{R})$ définie 
    pour tout $1\leq b,i\leq n$, par
    \[
        Q_{b,i} =
        \begin{cases}
        r_{b,k} & \text{si } b\ge 2 \text{ et } i = b-k+1 \text{ pour } 2\le k\le b\\
        -\lambda_b & \text{si } b\ge 2 \text{ et } i = b\\
        0 & \text{sinon}
        \end{cases}
    \]

    Le premier élément de sa diagonale, $Q_{1,1}$, est nul
    car l'état $1$ est absorbant donc $Q$ n'est pas inversible.
    En se restreignant à la sous-matrice $R = (Q_{i,j})_{2\leq i,j \leq n}$
    la matrice devient inversible et nous pouvons exprimer la densité de $\tau$.
    Posons $p_R(t) = (p_k(t))_{2\leq k \leq n}$
    où $p_k : t \geq 0 \mapsto \mathbb{P}(N_t = k \mid N_0=n)$. D'après
    la relation de Chapman-Kolmogorov, $p_R$ vérifie pour tout $t\geq 0$
    \[
    \begin{cases}
        p_R'(t) = p_R(t) R \\
        p_R(0) = (0,\dots,0,1)
    \end{cases}
    \iff p_R(t) = (0,\dots,0,1) e^{tR}
    \]
    Définissons la fonction de survie,
    $S : t \mapsto \mathbb{P}(\tau_n > t) = \mathbb{P}(N_t\neq 1 \mid N_0=n)
    = \sum_{k=2}^n p_k(t) = p_R(t) \cdot \textbf{1}$.
    Donc la densité de $\tau_n$ est donnée par, 
    \begin{equation}\label{eq:density-TMRCA}
        f_\tau : t\mapsto d_t (1-S(t))
        = -S'(t) = -p_R'(t) \cdot \textbf{1}
        = -p_R(t) R \cdot \textbf{1}
        = - (0,\dots,0,1) e^{tR} R \cdot \textbf{1}
    \end{equation}
   


    On remarque également que ce processus est défini 
    par $(\lambda_{b,k})_{I_n}$ avec
    $I_n \coloneq \{(b,k), 2\leq k \leq b \leq n \}$.
    Définissons pour $r \in \llbracket 0, n-2 \rrbracket$
    \[ 
    m_r : \Lambda \mapsto \int_0^1 x^r \Lambda(dx)
    \]

    En développant l'intégrande des taux de fusions, 
    pour tout $(b,k) \in I_n$,
    il existe $A_n \in \mathcal M_{I_n, n-1}(\mathbb{R})$ tel que, 
    \[
    \lambda_{b,k} = \sum_{r=0}^{n-2} A_{(b,k),r} m_r(\Lambda)
    \]
        
    Pour $n$ fixé, $Q$ est entièrement déterminée
    par $(m_r(\Lambda))_{0\leq r \leq n-2}$,
    l'espace
    des mesures de probabilité sur $[0,1]$ se réduit
    à une projection
    de dimension finie, $\mathbb{R}^{n-1}$, donc un espace bien plus petit.
    Autrement dit, 
    une infinité de mesures différentes
    deviennent indiscernables pour un processus considéré.
 
\begin{proposition}\label{prop:beta_indiscernable}
    Soit $n>1$ et $\Lambda_0^\alpha \coloneq \mathrm{Beta}(2-\alpha,\alpha)$ avec $\alpha \in ]1,2[$,
    de densité 
    \[w_\alpha : x\in [0,1] \longmapsto
    \frac{1}{B(2-\alpha,\alpha)} x^{1-\alpha}(1-x)^{\alpha-1}
    \]

    Soit $(J_{n})_{n\geq 0}$  les polynômes de Jacobi. 
    Pour tout $n\geq 0,
    J_{n}=\sum_{k=0}^{n}
        \binom{n+\alpha-1}{n-k+1}
        \binom{n-\alpha+1}{k}
        x^{n-k}(x-1)^k$
    est de degré $n$ et orthogonal à tous les polynômes de degré inférieur à $n-1$
    pour
    le produit scalaire 
    \cite{nevai1994generalized} % j'arrive pas à le mettre quelque part de bien

    \[
    \langle f,g\rangle_\alpha=\int_0^1 f(x)g(x) w_\alpha(x) dx 
    \]

      On pose
  \[
    M \coloneq \sup_{x\in[0,1]}|J_{n-1}(x)| \in ]0,+\infty[
    \qquad\text{et}\qquad
    \varepsilon_n := \frac{1}{M}
  \]

  Et on définit pour $0<\varepsilon< \varepsilon_n$, la mesure de
   probabilité $\Lambda_{\varepsilon}^\alpha \neq 
    \Lambda_0^\alpha$ par sa densité
    
    \[
    f_\varepsilon^\alpha : x \in [0,1] \longmapsto 
    \bigl(1+\varepsilon J_{n-1}(x)\bigr) w_\alpha(x)
    \]

    Alors, pour tout $0<\varepsilon<\varepsilon_n$,
    \[
    \tau_{\Lambda_{\varepsilon}^\alpha,n}
    \overset{\mathcal{L}}{=}
    \tau_{\Lambda_{0}^\alpha,n}
    \]
\end{proposition}
\begin{proof}
    Soit $\varepsilon<1/M$, montrons que $f_\varepsilon^\alpha$ est bien une densité.
    Sur $[0,1]$,
    $1+\varepsilon J_{n-1} \ge 1-\varepsilon M \ge 0$
    donc par produit de termes positifs $f_\varepsilon^\alpha \ge 0$.

    Puis, par orthogonalité de $J_{n-1}$ avec la constante
    $1 \in \mathbb{R}_{n-1}[X]$,
    % (comme $1$ est un polynôme de degré inférieur à $n-1$)

    \[
        \int_0^1 J_{n-1}(x) w_\alpha(x) dx = 0
    \]

    D'où,
    \[
        \int_0^1 f_\varepsilon^\alpha(x)dx
        = \int_0^1 w_\alpha(x)dx
          + \varepsilon \int_0^1 J_{n-1}(x)w_\alpha(x)dx
        = 1 + \varepsilon\cdot 0 = 1
    \]

    Ainsi $f_\varepsilon^\alpha$ est bien une densité sur $[0,1]$.

    Montrons à présent que les générateurs infinitésimaux de $\Lambda_0^\alpha$ et
    $\Lambda_\varepsilon^\alpha$ coïncident.
    Pour tout $r \in \llbracket 0, n-2 \rrbracket$, on a
    $X^r \in \mathbb R_{r}[X] \subset \mathbb{R}_{n-1}[X]$, ainsi par 
    orthogonalité de $J_{n-1}$,
    \[
        \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\,dx = 0
    \]

    Ainsi,
    $$
        m_r(\Lambda_\varepsilon^\alpha)
        = \int_0^1 x^r f_\varepsilon(x)\,dx
        = \int_0^1 x^r w_\alpha(x)\,dx
          + \varepsilon \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\,dx
        = m_r(\Lambda_0^\alpha)
    $$

   Les taux de fusions sont donc égaux entre ces mesures,
    d'où le résultat.
\end{proof}


%\begin{proposition}\label{}
%Soit $n>1$. Notons $P_k$ le polynôme de 
%Legendre de degré $k$. 
%Prenons $\Lambda_0$ la mesure uniforme 
%sur $[0,1]$. Pour tout $0<\varepsilon<1$,
%définissons la mesure de probabilité
%$\Lambda_\varepsilon \neq \Lambda_0$
%de densité sur $[0,1]$
%\[ 
%f_\varepsilon(x) = 1 + \varepsilon P_{n-1}(2x-1)
%\]
%
%Alors, pour tout $0<\varepsilon<1$, 
%\[
%\tau_{\Lambda_0} \overset{\mathcal{L}}{=} \tau_{\Lambda_\varepsilon}
%\]

%\end{proposition}
%\begin{proof}
%Montrons que pour tout $0<\varepsilon<1$,
%$f_\varepsilon$ est bien une densité de probabilité.
%Nous avons, par une analyse élémentaire que
%pour tout $k\geq 0$
%$|P_k|\leq 1$ sur $[-1,1]$
%\cite{WhittakerWatson1920} donc $f_\epsilon \geq 1-1\cdot\varepsilon \geq0 $.
%
%Rappelons que $(P_k)_{0\leq k \leq n}$ est une base orthogonale de 
%$\mathbb{R}_n[X]$ pour le produit scalaire 
%$f,g \mapsto \int_{-1}^1 f(x)g(x) dx$
%donc $\int_0^1 f_\varepsilon(x)dx =
%1+\frac{\varepsilon}{2} \int_{-1}^1 1\cdot P_n(x) dx = 1+0 = 1$. 
%Montrons à présent que le générateur infinitésimal
%de $\Lambda_0$ et $\Lambda_\varepsilon$ sont identiques.
%Soit $r \in \llbracket 0, n-2 \rrbracket$,
%puisque $X^r \in \mathbb{R}_{n-2}[X]
%\subset \mathbb{R}_{n-1}[X]$,
%\[
%m_r(\Lambda_\varepsilon)
%= \int_0^1 x^r f_\varepsilon(x) dx
%= \int_0^1 x^r dx + \varepsilon \int_0^1 x^r P_{n-1}(2x-1) dx
%= \frac{1}{r+1} + \varepsilon \cdot 0
%= m_r(\Lambda_0)
%\]
%
%Les taux de fusions sont donc égaux entre ces mesures,
%d'où le résultat.
%\end{proof}




% je sais pas d'où sort ce texte, je viens de le voir que maintenant : 24/11 22h36 - très chatGPT
% mais la justification est pas mal semble-t-il 
%     Dans la Proposition~3, le choix de la mesure $\Lambda_0 = \mathrm{Beta}(2-\alpha,\alpha)$ n’est pas
% arbitraire. Il est entièrement dicté par l’usage des polynômes de Jacobi. 
% En effet, ceux-ci sont orthogonaux pour le poids
% \[
% w_\alpha(x)=x^{1-\alpha}(1-x)^{\alpha-1},
% \]
% ce qui correspond exactement à la densité d’une loi Beta$(2-\alpha,\alpha)$. Ce choix garantit que le
% polynôme $J_{n-1}$ est orthogonal à tous les polynômes de degré $\le n-2$, de sorte que
% \[
% \int_0^1 x^r J_{n-1}(x) w_\alpha(x)\, dx = 0,
% \qquad 0 \le r \le n-2.
% \]
% Ainsi, la perturbation de densité $(1+\varepsilon J_{n-1}(x))w_\alpha(x)$ préserve les moments
% $m_0,\dots,m_{n-2}$ et produit une infinité de mesures distinctes induisant le même
% $\Lambda$-coalescent jusqu’à $n$ lignées.


Ainsi nous venons de construire une infinité de mesures différentes qui induisent le même processus de coalescence. 
On s'attendait à obtenir une infinité d'arbres généalogiques différents mais ceux-ci sont identiques en loi. 

Pour la construction nous utilisons les polynômes de Jacobi. Ce choix est motivé par la stucture de la mesure $\mathrm{Beta}(2-\alpha,\alpha)$ 
et du fait que les polynômes de Jacobi sont  orthogonaux pour le poids de la forme $x^\beta(1-x)^\gamma$ 
sur $[0,1]$ avec $\beta>-1,\gamma>-1$  (classiquement les polynômes de Jacobi sont définis sur $[-1,1]$ mais par un changement 
de variable on se ramène sur $[0,1]$). 

Le poids $w_\alpha$ implique que $\beta=\alpha-1, \gamma=1-\alpha$ et la condition sur $\beta$ et $\gamma$ impose que $\alpha \in ]0,2[$. 
Pour $0<\alpha<1$, le $\mathrm{Beta}(2-\alpha,\alpha)$ coalescent admet une fraction positive des individus qui reste sous forme de singletons à tout 
temps t>0 \cite{Berestycki_2008} ce qui a pour conséquence de rendre la profondeur des arbres généalogiques explosive quand n tend vers l'infini.
Par conséquent, nous gardons $\alpha \in ]1,2[$.



    


    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_foret_pas_grande_beta_combined}
        \caption{(a) \textit{Gauche}~:\textcolor{red}{TODO} comparaison des densités remarquablement
        différentes
        sur $[0,1]$, \(f_\varepsilon(x)=\bigl(1+\varepsilon J_{n-1}(x)\bigr)\,w_\alpha(x)\),
        pour $n=8$~: cas Beta($2-\alpha,\alpha$) (avec $\alpha \in \{1,1.4\}$)$\varepsilon=0$ et perturbation de Jacobi $\varepsilon \in \{0.2,0.8\}$
        (b) \textit{Droite}~: distribution empirique du TMRCA,
         issues de $n$ lignées, à partir
        de 8000 simulations, avec superposition de la densité théorique
        donnée par \eqref{eq:density-TMRCA}.}
            \textcolor{red}{Dire que beta a été simulé par trapeze ? montecarlo ? faire quelques
    phrase dessus} \label{fig:tmrca_foret}
    \end{figure}

    

    La construction que nous proposons permet de jouer avec les densités selon 2 paramètres.
    L'allure générale est dictée par $\alpha$ tandis que $\varepsilon$ intensifie l'amplitude des oscillations.
    Le choix de $\varepsilon$ est contraint d'être inférieur à $\varepsilon_n$.
    D'après \cite{Szego1939}, cette borne est assez restrictive,
    \[
    \varepsilon_n = \Theta_{n\to \infty}\Bigl(\frac{1}{n^{|\alpha-1|}}\Bigr)
    \]
    Dans la \autoref{fig:tmrca_foret} sur les graphiques du bas avec  $\alpha=1.4$,
    nous avons pour $n=8$ que $\varepsilon_n = 0.392062$. On remarque alors 
    que la distribution empirique vérifie bien la théorie pour $\varepsilon=0.2 < \varepsilon_n$
    et que pour $\varepsilon=0.8>\varepsilon_n$ nous ne convergeons déjà plus en loi. 

    Dans le cas $\alpha=1$, graphique du haut, nous obtenons la mesure uniforme et $\varepsilon_n$
    ne depend plus de $n$. Cela nous permet d'avoir des densités drastiquement 
    différentes pour une étude initiale de $n\gg 1$ lignées, comparées à une densité 
    avec $\alpha \neq 1$. Le choix de $\varepsilon$ est tout de même borné par $1$ 
    afin que notre densité soit positive, 


    %\begin{figure}[H]
%    \centering
%    \includesvg[width=\textwidth]{photo/plot_foret_pas_gde_beta}
%    \caption{(a) \textit{Gauche}~: comparaison des densités remarquablement
%    différentes
%    sur $[0,1]$, \(f_\varepsilon(x)=1+\varepsilon P_{n-1}(2x-1)\),
%    pour $n=10$~: cas uniforme
%    $\varepsilon=0$ et perturbation de Legendre $\varepsilon=0.5$
%    (b) \textit{Droite}~: distribution empirique du TMRCA,
%     issues de $n$ lignées, à partir
%    de 8000 simulations, avec superposition de la densité théorique
%    donnée par \eqref{eq:density-TMRCA}. Comme attendu les densités se
%    confondent.}
%\end{figure}

   

    \subsection{Silence, ça pousse}
    Précedemment nous avons brièvement parlé de la mesure uniforme en 
    prenant $\Lambda_0^\alpha$ avec $\alpha=1$.
    Ce modèle est connu sous le nom de Bolthausen-Sznitman
    et décèle un résultat incontournable.
    % \begin{theorem}[C. Goldschmidt \& J. B. Martin \cite{goldschmidt-martin-2005}]
    \begin{theorem}[Goldschmidt \& Martin~\cite{goldschmidt-martin-2005}]
        Soit $(N_t)_{t\geq 0}$ un Bolthausen-Sznitman coalescent.
        % Pour $n>0$,
        % $ \tau_n \coloneq \inf\{t\geq 0, N_0 = n, N_t=1\}$. 
        Alors, 

        \[
            \tau_n - \log(\log(n))  \xrightarrow[n\to\infty]{\mathcal{L}} \mathcal G
        \]
        
    où $\mathcal{G}$ est la loi de Gumbel, de densité $x\mapsto e^{-x - e^{-x}}$.

    \end{theorem}
    La preuve est omise car elle dépasse le cadre de ce rapport. Toutefois, 
    ce théorème renforce l'intuition qu'on a pu commencé à avoir
    à la \autoref{prop:surprise-TMRCA} puisqu'on a que,
    \[
    \lim_{n\to \infty}\mathbb{E}(\tau_n)
    = \lim_{n\to \infty} \log(\log(n)) + \mathbb{E}(\mathcal G)
    = \lim_{n\to \infty} \log(\log(n)) + \gamma
    = +\infty 
    \]
    où $\gamma$ est la constante d'Euler-Mascheroni. En effet,
    comme l'illustre la \autoref{fig:silence-ca-pousse},
    dès $n \approx 50$ on observe $\mathbb{E}(\tau_n) > 2$,
    surpassant la borne du modèle de Kingman $\eqref{eq:tmrca-kingman}$.
    Ainsi, la croissance de la hauteur des arbres généalogiques
    pour le modèle de Bolthausen-Sznitman est extrêmement lente 
    mais permet d'obtenir des arbres aussi grand que l'on souhaite 
    en moyenne.
    % Par exemple en se plaçant de la cadre asymptotique sur 
    % le nombre de lignées initiales $n$, 
    % si on souhaite avoir un arbre
    % de Bolthausen-Sznitman plus grand que celui de Kingman, dont sa hauteur
    % est majorée en moyenne par $2$, il faudrait résoudre 
    % $\log(\log(n))+\gamma \geq 2 \iff n \geq e^{e^{2-\gamma}} \approx 80 $

    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photo/plot_silence_ca_pousse}
        \caption{(a) \textit{Gauche}~:
        sous le modèle de Bolthausen-Sznitman ($\Lambda$ uniforme)
        nous déterminons les moyennes empiriques de
        $\mathbb{E}(\tau_n)$ (500 répétitions par $n \in 
        \{10, 20, 50, 100, 200, 500, 1e3,2e3,5e3,1e4\}$) comparées à
        l'approximation théorique $\gamma+\log\log n$ et aussi à
        Kingman \eqref{eq:tmrca-kingman}. (b) \textit{Droite}~: histogramme de
        $\tau_n-\log\log n$ pour $n=1000$ (5000 simulations) avec
        superposition de la densité de Gumbel $x\mapsto e^{-x-e^{-x}}$.}
        \label{fig:silence-ca-pousse}
    \end{figure}


\section{Application : Le Paradoxe de la Sardine Japonaise}


    Nous avons précédemment introduit la loi Beta$(2-\alpha, \alpha)$. 
    Celle-ci est cruciale en biologie des populations, car le modèle 
    de Kingman est insuffisant pour décrire certaines espèces.
    
    L'exemple de la sardine japonaise, analysé par Niwa et al. \cite{Niwa2016}, 
    illustre cette limite. Cette étude constitue une 
    démonstration empirique de la nécessité du $\Lambda$-coalescent 
    pour les espèces à stratégiedite de \textit{sweepstakes}, dans 
    lesquelles seuls quelques individus parmi des millions réussissent à
    produire une descendance massive, tandis que la majorité n'en laisse aucune. 

    \textcolor{red}{Là j'ai du mal car $x$, $C$ ne sont pas défini. Pourquoi une queue lourde 
    ça le modélise ? car si on veut que la sardine fasse beaucoup de bébés et d'autres de maniere négligeable
    il faudrait la masse vers 0 et une partie vers 1 mais en soit rien n'oblige à une "queue lourde", 
    peut-être il (me) manque un argument.}
    Mathématiquement, ce 
    comportement est modélisé par une distribution à queue lourde du nombre de descendants : l'article pose 
    explicitement que \textcolor{red}{pourquoi alpha n'a pas le droit d'etre en dessous de 1? peut-être
    cela viendra avec l'explication de la loi beta}
    \textcolor{red}{Vérifier du x->0 pour le Theta}
    \[
    \mathbb{P}(X \ge x) = \Theta_{x\to 0}( x^{-\alpha}), \qquad 1 < \alpha < 2,
    \]
    où $X$ désigne le nombre d'enfants produits par un individu. \textcolor{red}{Depuis le début 
    on parle de lignées et là on parle d'enfants. Donc il faut expliquer (en 4 mots, 1 phrases) 
    comment notre modèle de lignées permet de passer à cette interprétation}
    Dans cette plage de valeurs \textcolor{red}{Laquelle ?}, la loi possède 
    une variance infinie \textcolor{red}{var(X) ? si oui, à calculer.}, ce qui formalise 
    le caractère extrême des inégalités reproductives et traduit 
    rigoureusement le phénomène de \textit{sweepstakes}.


    \subsection{Le Conflit}

    Chez les animaux à haute fécondité, l'analyse génétique révèle 
    une contradiction si l'on reste dans le paradigme binaire 
    classique (Wright-Fisher ou Kingman): On observe un excès massif de mutations uniques (présentes sur un seul spécimen). 
    
    Sous Kingman, cette observation ne peut être interprétée que comme une 
    expansion démographique explosive et récente (dans cet article, 
    on estime que c'est de l'ordre de $10^3$).

    Si cette expansion était réelle, la distribution des distances 
    par paires (\textit{mismatch distribution}, définie comme la 
    distribution du nombre de allèles différentes entre 
    deux séquences pour l'ensemble des paires d'individus) 
    devrait former une loi de Poisson. 
    Or, les données montrent une distribution 
    en forme de ``L'' : un pic en zéro et une queue lourde.

    \subsection{Résolution par le Beta-Coalescent et Simulation}

    Le modèle Beta-coalescent (avec $\alpha \approx 1.3$, 
    valeur calculée empiriquement dans l'article) résout 
    ces problèmes. 
    
    Les fusions multiples fréquentes créent des 
    groupes d'individus identiques (le pic à 0), tout en laissant 
    survivre des lignées très anciennes (queue lourde), sans 
    recourir à l'hypothèse d'expansion explosive.

    Nous avons simulé cette distinction (Fig. \ref{fig:sardine_simu}) 
    en comparant un modèle de Kingman avec expansion 
    et un modèle Beta stationnaire (la réalité).

    \begin{figure}[H]
        \centering
        \includesvg[width=\textwidth]{photosweep}
        \caption{Distribution des discordances ($n=150$). 
        \textbf{En bleu (Kingman + Expansion)} : Une ``vague'' 
        créée par le phénomène de croissance démographique. 
        \textbf{En rouge (Sardine $\alpha=1.3$)} : Un pic de 
        clones et une queue de lignées persistantes 
        (résultat attendu).}
        \label{fig:sardine_simu}
    \end{figure}

    Pour valider cette simulation, nous la confrontons aux 
    résultats empiriques originaux (Fig. \ref{fig:niwa_real}). 
    On y retrouve exactement la dichotomie observée plus haut.

    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{photo/sardine.png}
        \caption{Comparaison des ajustements sur données réelles 
        (Source : Niwa et al., 2016). 
        \textbf{(a)} L'approche classique (Kingman), échouant à suivre les points noirs (données empiriques). 
        \textbf{(b)} Le modèle 
        $\Lambda$-coalescent. (Voir l'article original pour 
        les paramètres complets).}
        \label{fig:niwa_real}
    \end{figure}
    \subsection{Conséquence : Millions de sardines, même génome}

    Ce résultat invalide la relation linéaire entre taille de 
    recensement $N$ et taille efficace $N_e$. Sous le régime 
    Beta-coalescent, la dérive génétique est régie par une 
    loi de puissance :
    $$ N_e \propto N^{\alpha - 1} $$
    
    Pour la sardine ($\alpha \approx 1.3$), cela implique 
    $N_e \propto N^{0.3}$. Cette relation explique pourquoi 
    des populations marines gigantesques conservent une diversité 
    génétique très faible et instable. 
    
    Ce modèle permet non seulement d'expliquer un ancien paradoxe 
    chez les animaux à haute fécondité, mais sert également 
    d'outil pour estimer la taille effective d'une population.


 
\bibliographystyle{alpha}
\bibliography{ref}
\end{document}